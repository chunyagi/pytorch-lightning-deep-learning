{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder-Only Transformers (by batch)\n",
    "\n",
    "This notebook demonstrates how to build a very simple decoder-only Transformer (which can takes batches instead of a single example in a forward pass) from scratch. Decoder-only architectures, such as those powering large language models (LLMs) like ChatGPT, focus exclusively on the generative component of the Transformer. By working through this notebook, you'll see how these models process context and generate outputâ€”providing a foundation for understanding how modern LLMs operate under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import lightning as L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from vocabs to numbers as nn.Embedding can only take integers\n",
    "token_to_id = {\"what\": 0,\n",
    "               \"is\": 1,\n",
    "                \"statquest\": 2,\n",
    "                \"awesome\": 3,\n",
    "                \"<EOS>\": 4\n",
    "              }\n",
    "\n",
    "# Create a mapping from numbers back to vocabs to interpret the output from the transformer\n",
    "id_to_token = dict(map(reversed, token_to_id.items()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training dataset\n",
    "# As the input is going to be word embeddings, we only need the corresponding numbers from the mapping\n",
    "# The tokens used as inputs during training comes from 1. processing the prompt and 2. generating the output\n",
    "inputs = torch.tensor([[token_to_id[\"what\"],\n",
    "                        token_to_id[\"is\"],\n",
    "                        token_to_id[\"statquest\"],\n",
    "                        token_to_id[\"<EOS>\"],\n",
    "                        token_to_id[\"awesome\"]\n",
    "                        ], \n",
    "                        \n",
    "                        [token_to_id[\"statquest\"],\n",
    "                         token_to_id[\"is\"],\n",
    "                         token_to_id[\"what\"],\n",
    "                         token_to_id[\"<EOS>\"],\n",
    "                         token_to_id[\"awesome\"]]])\n",
    "\n",
    "labels = torch.tensor([[token_to_id[\"is\"],\n",
    "                         token_to_id[\"statquest\"],\n",
    "                         token_to_id[\"<EOS>\"],\n",
    "                         token_to_id[\"awesome\"],\n",
    "                         token_to_id[\"<EOS>\"]], \n",
    "                         \n",
    "                         [token_to_id[\"is\"],\n",
    "                          token_to_id[\"what\"],\n",
    "                          token_to_id[\"<EOS>\"],\n",
    "                          token_to_id[\"awesome\"],\n",
    "                          token_to_id[\"<EOS>\"]]])\n",
    "\n",
    "dataset = TensorDataset(inputs, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Position Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formula for the (standard, used in the paper **Attention is all you need**) position encoding is:  \n",
    "PE_(pos, 2i) = sin(pos / 10000^(2i / d_model))  \n",
    "PE_(pos, 2i+1) = cos(pos / 10000^(2i / d_model))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First try\n",
    "# class PositionEncoding(nn.Module):\n",
    "\n",
    "#     def __init__(self, batch_size=2, d_model=2, max_len=6):\n",
    "\n",
    "#         super().__init__()\n",
    "\n",
    "#         # pe stands for position encoding\n",
    "#         pe = torch.zeros(batch_size, max_len, d_model)\n",
    "\n",
    "#         # position is now a batch of 2D column matrices of size [batch_size, max_len, 1], e.g. [[0.], [1.], [2.]] * batch_size\n",
    "#         position = torch.arange(start=0, end=max_len, step=1).float().unsqueeze(1)\n",
    "#         # Add a leading batch dimension \n",
    "#         position = position.unsqueeze(0).expand(batch_size, max_len, 1)\n",
    "\n",
    "#         # Step is set to 2 because of \"2i\" in the formula, note that it is still a 1D tensor (no leading batch dimension is needed, it is still broadcastable)\n",
    "#         embedding_index = torch.arange(start=0, end=d_model, step=2).float()\n",
    "    \n",
    "#         # div_term is a row matrix (1D) with the same size as embedding_index\n",
    "#         div_term = torch.tensor(10000.)**(embedding_index / d_model)\n",
    "\n",
    "#         # Note: calculating the sin and cos values in this way only works when d_model is an even number, if d_model is odd, there will be a shape mismatch\n",
    "#         # E.g. [2, 3, 1] / [2] => [2, 3, 1] / [1, 1, 2] => results in a tensor of size [2, 3, 2]\n",
    "#         pe[:, :, 0::2] = torch.sin(position / div_term)\n",
    "#         pe[:, :, 1::2] = torch.cos(position / div_term)\n",
    "\n",
    "#         self.register_buffer('pe', pe)\n",
    "    \n",
    "\n",
    "#     def forward(self, word_embeddings):\n",
    "\n",
    "#         print(word_embeddings.shape)\n",
    "#         print(self.pe[:, :word_embeddings.size(1), :].shape)\n",
    "#         # Note: we need to return the position encodings across all the batches\n",
    "#         return word_embeddings + self.pe[:, :word_embeddings.size(1), :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model=2, max_len=6):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # pe stands for position encoding\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "\n",
    "        # position is a column matrix (2D) of size [max_len, 1], e.g. [[0.], [1.], [2.]]\n",
    "        position = torch.arange(start=0, end=max_len, step=1).float().unsqueeze(1)\n",
    "\n",
    "        # Step is set to 2 because of \"2i\" in the formula, note that it is a 1D tensor, e.g. [0., 2.] as each position can have multiple embedding values\n",
    "        embedding_index = torch.arange(start=0, end=d_model, step=2).float()\n",
    "\n",
    "        # div_term is a row matrix (1D) with the same size as embedding_index\n",
    "        div_term = torch.tensor(10000.)**(embedding_index / d_model)\n",
    "\n",
    "        # Note: calculating the sin and cos values in this way only works when d_model is an even number, if d_model is odd, there will be a shape mismatch\n",
    "        pe[:, 0::2] = torch.sin(position / div_term)\n",
    "        pe[:, 1::2] = torch.cos(position / div_term)\n",
    "\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "\n",
    "    def forward(self, word_embeddings):\n",
    "\n",
    "        # Note: the size of word_embeddings is [batch_size, input_len, d_model]\n",
    "        # and the size of self.pe[:word_embeddings.size(0), :] is [input_length, d_model]\n",
    "        # e.g. [2, 4, 2] + [4, 2] => [2, 4, 2] + [1, 4, 2] => [2, 4, 2]\n",
    "        return word_embeddings + self.pe[:word_embeddings.size(1), :]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "# pe = PositionEncoding(max_len=6, d_model=2)\n",
    "# we = nn.Embedding(num_embeddings=5, embedding_dim=2)\n",
    "\n",
    "# model_input = torch.tensor([token_to_id[\"what\"],\n",
    "#                             token_to_id[\"is\"],\n",
    "#                             token_to_id[\"statquest\"],\n",
    "#                             token_to_id[\"<EOS>\"]])\n",
    "\n",
    "# model_input = model_input.repeat(2, 1)\n",
    "\n",
    "# word_em = we(model_input)\n",
    "# pos_en = pe(word_em)\n",
    "# pos_en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masked Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model=2):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Create the weights associated with the query, key and value values\n",
    "        self.W_q = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        self.W_k = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        self.W_v = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "\n",
    "        self.batch_dim = 0\n",
    "        self.row_dim = 1\n",
    "        self.col_dim = 2\n",
    "\n",
    "    def forward(self, encodings_for_q, encodings_for_k, encodings_for_v, mask=None):\n",
    "\n",
    "        # Create the Q, K and V matrices\n",
    "        # Note: now the size of q, k and v is [batch_size, input_len, d_model] (for convenience, I will assume the batch_size = 2, seq_len = 4, d_model = 2)\n",
    "        q = self.W_q(encodings_for_q)\n",
    "        k = self.W_k(encodings_for_k)\n",
    "        v = self.W_v(encodings_for_v)\n",
    "\n",
    "        # Calculate the similarity score between the query values and key values\n",
    "        sims = torch.matmul(q, k.transpose(dim0=self.row_dim, dim1=self.col_dim))\n",
    "\n",
    "        # Scale the similarity score with the square root of d_model\n",
    "        # Broadcasting steps: [2, 4, 4] / [] => [2, 4, 4] / [1, 1, 1] => [2, 4, 4]\n",
    "        scaled_sims = sims / torch.tensor((k.size(self.col_dim))**0.5)\n",
    "        print(\"the dimension of scaled_sims: \", scaled_sims.shape)\n",
    "\n",
    "        device = scaled_sims.device\n",
    "        \n",
    "        # Mask the scaled similarity scores of the later tokens so that the earlier tokens can't cheat. Note: -1e9 is an approximation of negative infinity\n",
    "        if mask is not None:\n",
    "            # Move your mask to mps:0, or mask would live in cpu by default\n",
    "            mask = mask.to(device)\n",
    "            scaled_sims = scaled_sims.masked_fill(mask=mask, value=-1e9)\n",
    "            print(\"the dimension of scaled_sims after masking: \", scaled_sims.shape)\n",
    "\n",
    "        # Applying the softmax function to the scaled similarites determines the percentages of influence each token (in columns) should have on the others (in rows)\n",
    "        attention_percents = F.softmax(scaled_sims, dim=self.col_dim)\n",
    "        print(\"the dimension of attention_percents: \", attention_percents.shape)\n",
    "\n",
    "        # attention_scores are basically the contextualised embeddings\n",
    "        # The dimensions of the matrix multiplication: [2, 4, 4] * [2, 4, 2] => [2, 4, 2]\n",
    "        attention_scores = torch.matmul(attention_percents, v)\n",
    "        print(\"the dimension of attention_score: \", attention_scores.shape)\n",
    "\n",
    "        return attention_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder-only Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderOnlyTransformer(L.LightningModule):\n",
    "\n",
    "    def __init__(self, num_tokens, d_model, max_len, batch_size=2):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Word Embeddings\n",
    "        self.we = nn.Embedding(num_embeddings=num_tokens, embedding_dim=d_model)\n",
    "\n",
    "        # Position Encodings\n",
    "        self.pe = PositionEncoding(d_model=d_model, max_len=max_len)\n",
    "\n",
    "        # Masked Self-Attention\n",
    "        self.attention = Attention(d_model=d_model)\n",
    "\n",
    "        # Fully Connected layer\n",
    "        self.fc = nn.Linear(in_features=d_model, out_features=num_tokens)\n",
    "\n",
    "        # Calculate the loss with Cross Entropy; softmax is already included\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Specify batch size\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    # The size of token_ids is a 2D tensor [batch_size, seq_len], unlike nn.LSTM, which requires the size of the input tensor to be [seq_len, batch_size, input_size]\n",
    "    def forward(self, token_ids):\n",
    "\n",
    "        # Create word embeddings\n",
    "        word_embeddings = self.we(token_ids)\n",
    "\n",
    "        # Add position encodings to the word embeddings\n",
    "        position_encoded = self.pe(word_embeddings)\n",
    "\n",
    "        # Create a mask matrix for masking used in masked self attention\n",
    "        # The size of mask should be [seq_len, seq_len]\n",
    "        mask = torch.tril(torch.ones(token_ids.size(dim=1),token_ids.size(dim=1))) == 0\n",
    "        # Add a leading batch dimension to mask\n",
    "        mask = mask.unsqueeze(dim=0).expand(self.batch_size, token_ids.size(dim=1), token_ids.size(dim=1))\n",
    "        print(\"the dimension of mask: \", mask.shape)\n",
    "\n",
    "        # Masked Self-Attention\n",
    "        self_attention_values = self.attention(position_encoded,\n",
    "                                               position_encoded,\n",
    "                                               position_encoded,\n",
    "                                               mask=mask)\n",
    "\n",
    "        # Add residual connections\n",
    "        print(\"the dimension of position encoding: \", position_encoded.shape)\n",
    "        residual_connection_values = position_encoded + self_attention_values\n",
    "\n",
    "        # Run the residual connections through a fully connected layer\n",
    "        # fc_layer_out has the same size as residual_connected_values, which is a 2D tensor of size [seq_len, num_tokens]\n",
    "        fc_layer_out = self.fc(residual_connection_values)\n",
    "        print(\"the dimension of the fc layer output: \", fc_layer_out.shape)\n",
    "\n",
    "        # Return the fully connected layer\n",
    "        return fc_layer_out\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return Adam(self.parameters(), lr=0.1)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \n",
    "        # input_tokens is a 2D tensor of size [batch_size, seq_len]\n",
    "        input_tokens, labels = batch\n",
    "        print(batch)\n",
    "        print(input_tokens)\n",
    "        # outputs is fc_layer_out, so they share the same size\n",
    "        outputs = self.forward(input_tokens)\n",
    "        # Cross Entropy loss will automatically apply softmax to the outputs\n",
    "        # Sum the loss across the samples in the batch\n",
    "        loss = torch.sum(self.loss(outputs, labels))\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the input_length is:  4\n",
      "the dimension of mask:  torch.Size([2, 4, 4])\n",
      "the dimension of scaled_sims:  torch.Size([2, 4, 4])\n",
      "the dimension of scaled_sims after masking:  torch.Size([2, 4, 4])\n",
      "the dimension of attention_percents:  torch.Size([2, 4, 4])\n",
      "the dimension of attention_percents:  torch.Size([2, 4, 2])\n",
      "the dimension of position encoding:  torch.Size([2, 4, 2])\n",
      "the dimension of the fc layer output:  torch.Size([2, 4, 5])\n",
      "tensor([[[-2.0690,  1.2700,  0.7473,  1.8396, -0.7087],\n",
      "         [-1.1143,  1.0249,  0.9409,  0.5370, -0.4872],\n",
      "         [-0.5126,  0.8604,  0.9683, -0.2167, -0.3221],\n",
      "         [ 0.9594,  0.3338, -0.1442, -1.2195,  0.4005]],\n",
      "\n",
      "        [[-2.0690,  1.2700,  0.7473,  1.8396, -0.7087],\n",
      "         [-1.1143,  1.0249,  0.9409,  0.5370, -0.4872],\n",
      "         [-0.5126,  0.8604,  0.9683, -0.2167, -0.3221],\n",
      "         [ 0.9594,  0.3338, -0.1442, -1.2195,  0.4005]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([0, 0])\n"
     ]
    }
   ],
   "source": [
    "# Test if the forward pass is working with batch_size=2\n",
    "model = DecoderOnlyTransformer(num_tokens=len(token_to_id), d_model=2, max_len=6, batch_size=2)\n",
    "\n",
    "model_input = torch.tensor([token_to_id[\"what\"],\n",
    "                            token_to_id[\"is\"],\n",
    "                            token_to_id[\"statquest\"],\n",
    "                            token_to_id[\"<EOS>\"]])\n",
    "\n",
    "# Put one more (duplicated) sample in the batch\n",
    "# Note: -1 in the expand function means keep that dimension unchanged\n",
    "model_input = model_input.unsqueeze(0).expand(2, -1)\n",
    "\n",
    "# The dimension is changed to 1 now as the leading dimension refers to the batch_size\n",
    "input_length = model_input.size(dim=1)\n",
    "print(\"the input_length is: \", input_length)\n",
    "\n",
    "# predictions is the raw score output by the fulled connected layer\n",
    "predictions = model(model_input)\n",
    "# predictions is now a 3D tensor of size [batcg_size, seq_len (or input_length), num_tokens (or vocab_size)]\n",
    "print(predictions)\n",
    "\n",
    "# So now predicted_id is a 1D tensor of size [2] after argmax ([2, 5] => [2])\n",
    "predicted_id = torch.argmax(predictions[:,-1,:], dim=-1)\n",
    "print(predicted_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | we        | Embedding        | 10     | train\n",
      "1 | pe        | PositionEncoding | 0      | train\n",
      "2 | attention | Attention        | 12     | train\n",
      "3 | fc        | Linear           | 15     | train\n",
      "4 | loss      | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "37        Trainable params\n",
      "0         Non-trainable params\n",
      "37        Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd9660475fc543a9a10fa31963cdcf88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0'), tensor([[1, 2, 4, 3, 4],\n",
      "        [1, 0, 4, 3, 4]], device='mps:0')]\n",
      "tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0')\n",
      "the dimension of mask:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims after masking:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 2])\n",
      "the dimension of position encoding:  torch.Size([2, 5, 2])\n",
      "the dimension of the fc layer output:  torch.Size([2, 5, 5])\n",
      "[tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0'), tensor([[1, 2, 4, 3, 4],\n",
      "        [1, 0, 4, 3, 4]], device='mps:0')]\n",
      "tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0')\n",
      "the dimension of mask:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims after masking:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 2])\n",
      "the dimension of position encoding:  torch.Size([2, 5, 2])\n",
      "the dimension of the fc layer output:  torch.Size([2, 5, 5])\n",
      "[tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0'), tensor([[1, 2, 4, 3, 4],\n",
      "        [1, 0, 4, 3, 4]], device='mps:0')]\n",
      "tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0')\n",
      "the dimension of mask:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims after masking:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 2])\n",
      "the dimension of position encoding:  torch.Size([2, 5, 2])\n",
      "the dimension of the fc layer output:  torch.Size([2, 5, 5])\n",
      "[tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0'), tensor([[1, 2, 4, 3, 4],\n",
      "        [1, 0, 4, 3, 4]], device='mps:0')]\n",
      "tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0')\n",
      "the dimension of mask:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims after masking:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 2])\n",
      "the dimension of position encoding:  torch.Size([2, 5, 2])\n",
      "the dimension of the fc layer output:  torch.Size([2, 5, 5])\n",
      "[tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0'), tensor([[1, 2, 4, 3, 4],\n",
      "        [1, 0, 4, 3, 4]], device='mps:0')]\n",
      "tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0')\n",
      "the dimension of mask:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims after masking:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 2])\n",
      "the dimension of position encoding:  torch.Size([2, 5, 2])\n",
      "the dimension of the fc layer output:  torch.Size([2, 5, 5])\n",
      "[tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0'), tensor([[1, 2, 4, 3, 4],\n",
      "        [1, 0, 4, 3, 4]], device='mps:0')]\n",
      "tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0')\n",
      "the dimension of mask:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims after masking:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 2])\n",
      "the dimension of position encoding:  torch.Size([2, 5, 2])\n",
      "the dimension of the fc layer output:  torch.Size([2, 5, 5])\n",
      "[tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0'), tensor([[1, 2, 4, 3, 4],\n",
      "        [1, 0, 4, 3, 4]], device='mps:0')]\n",
      "tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0')\n",
      "the dimension of mask:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims after masking:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 2])\n",
      "the dimension of position encoding:  torch.Size([2, 5, 2])\n",
      "the dimension of the fc layer output:  torch.Size([2, 5, 5])\n",
      "[tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0'), tensor([[1, 2, 4, 3, 4],\n",
      "        [1, 0, 4, 3, 4]], device='mps:0')]\n",
      "tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0')\n",
      "the dimension of mask:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims after masking:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 2])\n",
      "the dimension of position encoding:  torch.Size([2, 5, 2])\n",
      "the dimension of the fc layer output:  torch.Size([2, 5, 5])\n",
      "[tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0'), tensor([[1, 2, 4, 3, 4],\n",
      "        [1, 0, 4, 3, 4]], device='mps:0')]\n",
      "tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0')\n",
      "the dimension of mask:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims after masking:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 2])\n",
      "the dimension of position encoding:  torch.Size([2, 5, 2])\n",
      "the dimension of the fc layer output:  torch.Size([2, 5, 5])\n",
      "[tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0'), tensor([[1, 2, 4, 3, 4],\n",
      "        [1, 0, 4, 3, 4]], device='mps:0')]\n",
      "tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0')\n",
      "the dimension of mask:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims after masking:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 2])\n",
      "the dimension of position encoding:  torch.Size([2, 5, 2])\n",
      "the dimension of the fc layer output:  torch.Size([2, 5, 5])\n",
      "[tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0'), tensor([[1, 2, 4, 3, 4],\n",
      "        [1, 0, 4, 3, 4]], device='mps:0')]\n",
      "tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0')\n",
      "the dimension of mask:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims after masking:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 2])\n",
      "the dimension of position encoding:  torch.Size([2, 5, 2])\n",
      "the dimension of the fc layer output:  torch.Size([2, 5, 5])\n",
      "[tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0'), tensor([[1, 2, 4, 3, 4],\n",
      "        [1, 0, 4, 3, 4]], device='mps:0')]\n",
      "tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0')\n",
      "the dimension of mask:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims after masking:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 2])\n",
      "the dimension of position encoding:  torch.Size([2, 5, 2])\n",
      "the dimension of the fc layer output:  torch.Size([2, 5, 5])\n",
      "[tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0'), tensor([[1, 2, 4, 3, 4],\n",
      "        [1, 0, 4, 3, 4]], device='mps:0')]\n",
      "tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0')\n",
      "the dimension of mask:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims after masking:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 2])\n",
      "the dimension of position encoding:  torch.Size([2, 5, 2])\n",
      "the dimension of the fc layer output:  torch.Size([2, 5, 5])\n",
      "[tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0'), tensor([[1, 2, 4, 3, 4],\n",
      "        [1, 0, 4, 3, 4]], device='mps:0')]\n",
      "tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0')\n",
      "the dimension of mask:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims after masking:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 2])\n",
      "the dimension of position encoding:  torch.Size([2, 5, 2])\n",
      "the dimension of the fc layer output:  torch.Size([2, 5, 5])\n",
      "[tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0'), tensor([[1, 2, 4, 3, 4],\n",
      "        [1, 0, 4, 3, 4]], device='mps:0')]\n",
      "tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0')\n",
      "the dimension of mask:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims after masking:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 2])\n",
      "the dimension of position encoding:  torch.Size([2, 5, 2])\n",
      "the dimension of the fc layer output:  torch.Size([2, 5, 5])\n",
      "[tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0'), tensor([[1, 2, 4, 3, 4],\n",
      "        [1, 0, 4, 3, 4]], device='mps:0')]\n",
      "tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0')\n",
      "the dimension of mask:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims after masking:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 2])\n",
      "the dimension of position encoding:  torch.Size([2, 5, 2])\n",
      "the dimension of the fc layer output:  torch.Size([2, 5, 5])\n",
      "[tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0'), tensor([[1, 2, 4, 3, 4],\n",
      "        [1, 0, 4, 3, 4]], device='mps:0')]\n",
      "tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0')\n",
      "the dimension of mask:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims after masking:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 2])\n",
      "the dimension of position encoding:  torch.Size([2, 5, 2])\n",
      "the dimension of the fc layer output:  torch.Size([2, 5, 5])\n",
      "[tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0'), tensor([[1, 2, 4, 3, 4],\n",
      "        [1, 0, 4, 3, 4]], device='mps:0')]\n",
      "tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0')\n",
      "the dimension of mask:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims after masking:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 2])\n",
      "the dimension of position encoding:  torch.Size([2, 5, 2])\n",
      "the dimension of the fc layer output:  torch.Size([2, 5, 5])\n",
      "[tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0'), tensor([[1, 2, 4, 3, 4],\n",
      "        [1, 0, 4, 3, 4]], device='mps:0')]\n",
      "tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0')\n",
      "the dimension of mask:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims after masking:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 2])\n",
      "the dimension of position encoding:  torch.Size([2, 5, 2])\n",
      "the dimension of the fc layer output:  torch.Size([2, 5, 5])\n",
      "[tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0'), tensor([[1, 2, 4, 3, 4],\n",
      "        [1, 0, 4, 3, 4]], device='mps:0')]\n",
      "tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0')\n",
      "the dimension of mask:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims after masking:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 2])\n",
      "the dimension of position encoding:  torch.Size([2, 5, 2])\n",
      "the dimension of the fc layer output:  torch.Size([2, 5, 5])\n",
      "[tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0'), tensor([[1, 2, 4, 3, 4],\n",
      "        [1, 0, 4, 3, 4]], device='mps:0')]\n",
      "tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0')\n",
      "the dimension of mask:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims after masking:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 2])\n",
      "the dimension of position encoding:  torch.Size([2, 5, 2])\n",
      "the dimension of the fc layer output:  torch.Size([2, 5, 5])\n",
      "[tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0'), tensor([[1, 2, 4, 3, 4],\n",
      "        [1, 0, 4, 3, 4]], device='mps:0')]\n",
      "tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0')\n",
      "the dimension of mask:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims after masking:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 2])\n",
      "the dimension of position encoding:  torch.Size([2, 5, 2])\n",
      "the dimension of the fc layer output:  torch.Size([2, 5, 5])\n",
      "[tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0'), tensor([[1, 2, 4, 3, 4],\n",
      "        [1, 0, 4, 3, 4]], device='mps:0')]\n",
      "tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0')\n",
      "the dimension of mask:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims after masking:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 2])\n",
      "the dimension of position encoding:  torch.Size([2, 5, 2])\n",
      "the dimension of the fc layer output:  torch.Size([2, 5, 5])\n",
      "[tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0'), tensor([[1, 2, 4, 3, 4],\n",
      "        [1, 0, 4, 3, 4]], device='mps:0')]\n",
      "tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0')\n",
      "the dimension of mask:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims after masking:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 2])\n",
      "the dimension of position encoding:  torch.Size([2, 5, 2])\n",
      "the dimension of the fc layer output:  torch.Size([2, 5, 5])\n",
      "[tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0'), tensor([[1, 2, 4, 3, 4],\n",
      "        [1, 0, 4, 3, 4]], device='mps:0')]\n",
      "tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0')\n",
      "the dimension of mask:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims after masking:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 2])\n",
      "the dimension of position encoding:  torch.Size([2, 5, 2])\n",
      "the dimension of the fc layer output:  torch.Size([2, 5, 5])\n",
      "[tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0'), tensor([[1, 2, 4, 3, 4],\n",
      "        [1, 0, 4, 3, 4]], device='mps:0')]\n",
      "tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0')\n",
      "the dimension of mask:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims after masking:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 2])\n",
      "the dimension of position encoding:  torch.Size([2, 5, 2])\n",
      "the dimension of the fc layer output:  torch.Size([2, 5, 5])\n",
      "[tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0'), tensor([[1, 2, 4, 3, 4],\n",
      "        [1, 0, 4, 3, 4]], device='mps:0')]\n",
      "tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0')\n",
      "the dimension of mask:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims after masking:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 2])\n",
      "the dimension of position encoding:  torch.Size([2, 5, 2])\n",
      "the dimension of the fc layer output:  torch.Size([2, 5, 5])\n",
      "[tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0'), tensor([[1, 2, 4, 3, 4],\n",
      "        [1, 0, 4, 3, 4]], device='mps:0')]\n",
      "tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0')\n",
      "the dimension of mask:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims after masking:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 2])\n",
      "the dimension of position encoding:  torch.Size([2, 5, 2])\n",
      "the dimension of the fc layer output:  torch.Size([2, 5, 5])\n",
      "[tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0'), tensor([[1, 2, 4, 3, 4],\n",
      "        [1, 0, 4, 3, 4]], device='mps:0')]\n",
      "tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0')\n",
      "the dimension of mask:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims after masking:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 2])\n",
      "the dimension of position encoding:  torch.Size([2, 5, 2])\n",
      "the dimension of the fc layer output:  torch.Size([2, 5, 5])\n",
      "[tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0'), tensor([[1, 2, 4, 3, 4],\n",
      "        [1, 0, 4, 3, 4]], device='mps:0')]\n",
      "tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0')\n",
      "the dimension of mask:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims after masking:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 2])\n",
      "the dimension of position encoding:  torch.Size([2, 5, 2])\n",
      "the dimension of the fc layer output:  torch.Size([2, 5, 5])\n",
      "[tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0'), tensor([[1, 2, 4, 3, 4],\n",
      "        [1, 0, 4, 3, 4]], device='mps:0')]\n",
      "tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0')\n",
      "the dimension of mask:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims after masking:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 2])\n",
      "the dimension of position encoding:  torch.Size([2, 5, 2])\n",
      "the dimension of the fc layer output:  torch.Size([2, 5, 5])\n",
      "[tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0'), tensor([[1, 2, 4, 3, 4],\n",
      "        [1, 0, 4, 3, 4]], device='mps:0')]\n",
      "tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0')\n",
      "the dimension of mask:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims after masking:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 2])\n",
      "the dimension of position encoding:  torch.Size([2, 5, 2])\n",
      "the dimension of the fc layer output:  torch.Size([2, 5, 5])\n",
      "[tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0'), tensor([[1, 2, 4, 3, 4],\n",
      "        [1, 0, 4, 3, 4]], device='mps:0')]\n",
      "tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0')\n",
      "the dimension of mask:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims after masking:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 2])\n",
      "the dimension of position encoding:  torch.Size([2, 5, 2])\n",
      "the dimension of the fc layer output:  torch.Size([2, 5, 5])\n",
      "[tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0'), tensor([[1, 2, 4, 3, 4],\n",
      "        [1, 0, 4, 3, 4]], device='mps:0')]\n",
      "tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0')\n",
      "the dimension of mask:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims after masking:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 2])\n",
      "the dimension of position encoding:  torch.Size([2, 5, 2])\n",
      "the dimension of the fc layer output:  torch.Size([2, 5, 5])\n",
      "[tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0'), tensor([[1, 2, 4, 3, 4],\n",
      "        [1, 0, 4, 3, 4]], device='mps:0')]\n",
      "tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0')\n",
      "the dimension of mask:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims after masking:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 2])\n",
      "the dimension of position encoding:  torch.Size([2, 5, 2])\n",
      "the dimension of the fc layer output:  torch.Size([2, 5, 5])\n",
      "[tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0'), tensor([[1, 2, 4, 3, 4],\n",
      "        [1, 0, 4, 3, 4]], device='mps:0')]\n",
      "tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0')\n",
      "the dimension of mask:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims after masking:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 2])\n",
      "the dimension of position encoding:  torch.Size([2, 5, 2])\n",
      "the dimension of the fc layer output:  torch.Size([2, 5, 5])\n",
      "[tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0'), tensor([[1, 2, 4, 3, 4],\n",
      "        [1, 0, 4, 3, 4]], device='mps:0')]\n",
      "tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0')\n",
      "the dimension of mask:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims after masking:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 2])\n",
      "the dimension of position encoding:  torch.Size([2, 5, 2])\n",
      "the dimension of the fc layer output:  torch.Size([2, 5, 5])\n",
      "[tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0'), tensor([[1, 2, 4, 3, 4],\n",
      "        [1, 0, 4, 3, 4]], device='mps:0')]\n",
      "tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0')\n",
      "the dimension of mask:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims after masking:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 2])\n",
      "the dimension of position encoding:  torch.Size([2, 5, 2])\n",
      "the dimension of the fc layer output:  torch.Size([2, 5, 5])\n",
      "[tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0'), tensor([[1, 2, 4, 3, 4],\n",
      "        [1, 0, 4, 3, 4]], device='mps:0')]\n",
      "tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0')\n",
      "the dimension of mask:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims after masking:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 2])\n",
      "the dimension of position encoding:  torch.Size([2, 5, 2])\n",
      "the dimension of the fc layer output:  torch.Size([2, 5, 5])\n",
      "[tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0'), tensor([[1, 2, 4, 3, 4],\n",
      "        [1, 0, 4, 3, 4]], device='mps:0')]\n",
      "tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0')\n",
      "the dimension of mask:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims after masking:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 2])\n",
      "the dimension of position encoding:  torch.Size([2, 5, 2])\n",
      "the dimension of the fc layer output:  torch.Size([2, 5, 5])\n",
      "[tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0'), tensor([[1, 2, 4, 3, 4],\n",
      "        [1, 0, 4, 3, 4]], device='mps:0')]\n",
      "tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0')\n",
      "the dimension of mask:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims after masking:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 2])\n",
      "the dimension of position encoding:  torch.Size([2, 5, 2])\n",
      "the dimension of the fc layer output:  torch.Size([2, 5, 5])\n",
      "[tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0'), tensor([[1, 2, 4, 3, 4],\n",
      "        [1, 0, 4, 3, 4]], device='mps:0')]\n",
      "tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0')\n",
      "the dimension of mask:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims after masking:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 2])\n",
      "the dimension of position encoding:  torch.Size([2, 5, 2])\n",
      "the dimension of the fc layer output:  torch.Size([2, 5, 5])\n",
      "[tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0'), tensor([[1, 2, 4, 3, 4],\n",
      "        [1, 0, 4, 3, 4]], device='mps:0')]\n",
      "tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0')\n",
      "the dimension of mask:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims after masking:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 2])\n",
      "the dimension of position encoding:  torch.Size([2, 5, 2])\n",
      "the dimension of the fc layer output:  torch.Size([2, 5, 5])\n",
      "[tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0'), tensor([[1, 2, 4, 3, 4],\n",
      "        [1, 0, 4, 3, 4]], device='mps:0')]\n",
      "tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0')\n",
      "the dimension of mask:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims after masking:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 2])\n",
      "the dimension of position encoding:  torch.Size([2, 5, 2])\n",
      "the dimension of the fc layer output:  torch.Size([2, 5, 5])\n",
      "[tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0'), tensor([[1, 2, 4, 3, 4],\n",
      "        [1, 0, 4, 3, 4]], device='mps:0')]\n",
      "tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0')\n",
      "the dimension of mask:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims after masking:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 2])\n",
      "the dimension of position encoding:  torch.Size([2, 5, 2])\n",
      "the dimension of the fc layer output:  torch.Size([2, 5, 5])\n",
      "[tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0'), tensor([[1, 2, 4, 3, 4],\n",
      "        [1, 0, 4, 3, 4]], device='mps:0')]\n",
      "tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0')\n",
      "the dimension of mask:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims after masking:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 2])\n",
      "the dimension of position encoding:  torch.Size([2, 5, 2])\n",
      "the dimension of the fc layer output:  torch.Size([2, 5, 5])\n",
      "[tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0'), tensor([[1, 2, 4, 3, 4],\n",
      "        [1, 0, 4, 3, 4]], device='mps:0')]\n",
      "tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0')\n",
      "the dimension of mask:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims after masking:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 2])\n",
      "the dimension of position encoding:  torch.Size([2, 5, 2])\n",
      "the dimension of the fc layer output:  torch.Size([2, 5, 5])\n",
      "[tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0'), tensor([[1, 2, 4, 3, 4],\n",
      "        [1, 0, 4, 3, 4]], device='mps:0')]\n",
      "tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0')\n",
      "the dimension of mask:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims after masking:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 2])\n",
      "the dimension of position encoding:  torch.Size([2, 5, 2])\n",
      "the dimension of the fc layer output:  torch.Size([2, 5, 5])\n",
      "[tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0'), tensor([[1, 2, 4, 3, 4],\n",
      "        [1, 0, 4, 3, 4]], device='mps:0')]\n",
      "tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0')\n",
      "the dimension of mask:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims after masking:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 2])\n",
      "the dimension of position encoding:  torch.Size([2, 5, 2])\n",
      "the dimension of the fc layer output:  torch.Size([2, 5, 5])\n",
      "[tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0'), tensor([[1, 2, 4, 3, 4],\n",
      "        [1, 0, 4, 3, 4]], device='mps:0')]\n",
      "tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0')\n",
      "the dimension of mask:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims after masking:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 2])\n",
      "the dimension of position encoding:  torch.Size([2, 5, 2])\n",
      "the dimension of the fc layer output:  torch.Size([2, 5, 5])\n",
      "[tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0'), tensor([[1, 2, 4, 3, 4],\n",
      "        [1, 0, 4, 3, 4]], device='mps:0')]\n",
      "tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0')\n",
      "the dimension of mask:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims after masking:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 2])\n",
      "the dimension of position encoding:  torch.Size([2, 5, 2])\n",
      "the dimension of the fc layer output:  torch.Size([2, 5, 5])\n",
      "[tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0'), tensor([[1, 2, 4, 3, 4],\n",
      "        [1, 0, 4, 3, 4]], device='mps:0')]\n",
      "tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0')\n",
      "the dimension of mask:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims after masking:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 2])\n",
      "the dimension of position encoding:  torch.Size([2, 5, 2])\n",
      "the dimension of the fc layer output:  torch.Size([2, 5, 5])\n",
      "[tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0'), tensor([[1, 2, 4, 3, 4],\n",
      "        [1, 0, 4, 3, 4]], device='mps:0')]\n",
      "tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0')\n",
      "the dimension of mask:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims after masking:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 2])\n",
      "the dimension of position encoding:  torch.Size([2, 5, 2])\n",
      "the dimension of the fc layer output:  torch.Size([2, 5, 5])\n",
      "[tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0'), tensor([[1, 2, 4, 3, 4],\n",
      "        [1, 0, 4, 3, 4]], device='mps:0')]\n",
      "tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0')\n",
      "the dimension of mask:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims after masking:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 2])\n",
      "the dimension of position encoding:  torch.Size([2, 5, 2])\n",
      "the dimension of the fc layer output:  torch.Size([2, 5, 5])\n",
      "[tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0'), tensor([[1, 2, 4, 3, 4],\n",
      "        [1, 0, 4, 3, 4]], device='mps:0')]\n",
      "tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0')\n",
      "the dimension of mask:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims after masking:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 2])\n",
      "the dimension of position encoding:  torch.Size([2, 5, 2])\n",
      "the dimension of the fc layer output:  torch.Size([2, 5, 5])\n",
      "[tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0'), tensor([[1, 2, 4, 3, 4],\n",
      "        [1, 0, 4, 3, 4]], device='mps:0')]\n",
      "tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0')\n",
      "the dimension of mask:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims after masking:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 2])\n",
      "the dimension of position encoding:  torch.Size([2, 5, 2])\n",
      "the dimension of the fc layer output:  torch.Size([2, 5, 5])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=60` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0'), tensor([[1, 2, 4, 3, 4],\n",
      "        [1, 0, 4, 3, 4]], device='mps:0')]\n",
      "tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0')\n",
      "the dimension of mask:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims after masking:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 2])\n",
      "the dimension of position encoding:  torch.Size([2, 5, 2])\n",
      "the dimension of the fc layer output:  torch.Size([2, 5, 5])\n",
      "[tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0'), tensor([[1, 2, 4, 3, 4],\n",
      "        [1, 0, 4, 3, 4]], device='mps:0')]\n",
      "tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0')\n",
      "the dimension of mask:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims after masking:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 2])\n",
      "the dimension of position encoding:  torch.Size([2, 5, 2])\n",
      "the dimension of the fc layer output:  torch.Size([2, 5, 5])\n",
      "[tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0'), tensor([[1, 2, 4, 3, 4],\n",
      "        [1, 0, 4, 3, 4]], device='mps:0')]\n",
      "tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0')\n",
      "the dimension of mask:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims after masking:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 2])\n",
      "the dimension of position encoding:  torch.Size([2, 5, 2])\n",
      "the dimension of the fc layer output:  torch.Size([2, 5, 5])\n",
      "[tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0'), tensor([[1, 2, 4, 3, 4],\n",
      "        [1, 0, 4, 3, 4]], device='mps:0')]\n",
      "tensor([[0, 1, 2, 4, 3],\n",
      "        [2, 1, 0, 4, 3]], device='mps:0')\n",
      "the dimension of mask:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims:  torch.Size([2, 5, 5])\n",
      "the dimension of scaled_sims after masking:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 5])\n",
      "the dimension of attention_percents:  torch.Size([2, 5, 2])\n",
      "the dimension of position encoding:  torch.Size([2, 5, 2])\n",
      "the dimension of the fc layer output:  torch.Size([2, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(max_epochs=30)\n",
    "trainer.fit(model, train_dataloaders=dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the input_length is:  4\n",
      "the dimension of mask:  torch.Size([2, 4, 4])\n",
      "the dimension of scaled_sims:  torch.Size([2, 4, 4])\n",
      "the dimension of scaled_sims after masking:  torch.Size([2, 4, 4])\n",
      "the dimension of attention_percents:  torch.Size([2, 4, 4])\n",
      "the dimension of attention_percents:  torch.Size([2, 4, 2])\n",
      "the dimension of position encoding:  torch.Size([2, 4, 2])\n",
      "the dimension of the fc layer output:  torch.Size([2, 4, 5])\n",
      "tensor([[[ -5.0525,   4.2033,  -4.8357,   0.4521,  -5.2534],\n",
      "         [ 10.5997,   9.8727,  -1.4766, -11.3479,   1.2655],\n",
      "         [ -3.9753,  17.5173, -13.3411,  -5.6371, -15.5727],\n",
      "         [-24.4906, -19.2599,   2.0944,  21.8119,   0.3338]],\n",
      "\n",
      "        [[ -5.0525,   4.2033,  -4.8357,   0.4521,  -5.2534],\n",
      "         [ 10.5997,   9.8727,  -1.4766, -11.3479,   1.2655],\n",
      "         [ -3.9753,  17.5173, -13.3411,  -5.6371, -15.5727],\n",
      "         [-24.4906, -19.2599,   2.0944,  21.8119,   0.3338]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([3, 3])\n"
     ]
    }
   ],
   "source": [
    "model_input = torch.tensor([token_to_id[\"what\"],\n",
    "                            token_to_id[\"is\"],\n",
    "                            token_to_id[\"statquest\"],\n",
    "                            token_to_id[\"<EOS>\"]])\n",
    "\n",
    "# Put one more (duplicated) sample in the batch\n",
    "# Note: -1 in the expand function means keep that dimension unchanged\n",
    "model_input = model_input.unsqueeze(0).expand(2,-1)\n",
    "\n",
    "# The dimension is changed to 1 now as the leading dimension refers to the batch_size\n",
    "input_length = model_input.size(dim=1)\n",
    "print(\"the input_length is: \", input_length)\n",
    "\n",
    "# predictions is the raw score output by the fulled connected layer\n",
    "predictions = model(model_input)\n",
    "# predictions is now a 3D tensor of size [batch_size, seq_len (or input_length), num_tokens (or vocab_size)]\n",
    "print(predictions)\n",
    "\n",
    "# So now predicted_id is a 1D tensor of size [2] after argmax ([2, 5] => [2])\n",
    "predicted_id = torch.argmax(predictions[:,-1,:], dim=-1)\n",
    "print(predicted_id)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
