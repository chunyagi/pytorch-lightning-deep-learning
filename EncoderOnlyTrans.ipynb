{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replicate the Decoder-Only Transformers architecture with PyTorch + Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import lightning as L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup device-agnostic code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '<CLS>',\n",
       " 1: '<EOS>',\n",
       " 2: 'machine',\n",
       " 3: 'learning',\n",
       " 4: 'i',\n",
       " 5: 'hate',\n",
       " 6: 'is',\n",
       " 7: 'fun'}"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a mapping from vocabs to numbers to mimic the tokenizer\n",
    "token_to_id = {\n",
    "    \"<CLS>\": 0, # mimic the special class token in BERT\n",
    "    \"<EOS>\": 1, # <SEP> in BERT\n",
    "    \"machine\": 2,\n",
    "    \"learning\": 3,\n",
    "    \"i\": 4,\n",
    "    \"hate\": 5,\n",
    "    \"is\": 6,\n",
    "    \"fun\": 7\n",
    "}\n",
    "\n",
    "# Create a mapping from numbers back to vocabs for interpreting the output from the transformer later\n",
    "id_to_token = dict(map(reversed, token_to_id.items()))\n",
    "id_to_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 2, 3, 6, 7, 1],\n",
       "         [0, 4, 5, 2, 3, 1]]),\n",
       " tensor([1, 0]))"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = torch.tensor([\n",
    "    [\n",
    "        token_to_id[\"<CLS>\"],\n",
    "        token_to_id[\"machine\"],\n",
    "        token_to_id[\"learning\"],\n",
    "        token_to_id[\"is\"],\n",
    "        token_to_id[\"fun\"],\n",
    "        token_to_id[\"<EOS>\"]\n",
    "    ],\n",
    "    \n",
    "    [\n",
    "        token_to_id[\"<CLS>\"],\n",
    "        token_to_id[\"i\"],\n",
    "        token_to_id[\"hate\"],\n",
    "        token_to_id[\"machine\"],\n",
    "        token_to_id[\"learning\"],\n",
    "        token_to_id[\"<EOS>\"]\n",
    "    ]\n",
    "])\n",
    "\n",
    "labels = torch.tensor([1, 0])\n",
    "inputs, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create position encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the formula used in the paper *Attention is all you need* position encoding is:\n",
    "* PE_(pos, 2i) = sin(pos / 10000^(2i / d_model))\n",
    "* PE_(pos, 2i+1) = cos(pos / 10000^(2i / d_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model=2, max_len=6):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "\n",
    "        position = torch.arange(start=0, end=max_len, step=1).float().unsqueeze(1)\n",
    "\n",
    "        embedding_index = torch.arange(start=0, end=d_model, step=2).float()\n",
    "\n",
    "        div_term = torch.tensor(10000)**(embedding_index / d_model)\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position / div_term)\n",
    "        pe[:, 1::2] = torch.cos(position / div_term)\n",
    "\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "    \n",
    "    def forward(self, word_embeddings):\n",
    "\n",
    "        return word_embeddings + self.pe[:word_embeddings.size(0), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Self-Attention layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model=2):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.W_q = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        self.W_k = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        self.W_v = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "\n",
    "        self.row_dim = 0\n",
    "        self.col_dim = 1\n",
    "\n",
    "    def forward(self, encodings_for_q, encodings_for_k, encodings_for_v, mask=None, device=device):\n",
    "\n",
    "        # Create the Q, K and V matrices\n",
    "        q = self.W_q(encodings_for_q)\n",
    "        k = self.W_k(encodings_for_k)\n",
    "        v = self.W_v(encodings_for_v)\n",
    "\n",
    "        # Calculate the similarity score between the queries and values\n",
    "        sims = torch.matmul(q, k.transpose(dim0=self.row_dim, dim1=self.col_dim))\n",
    "\n",
    "        # Scale the similarity score with the square root of d_model\n",
    "        scaled_sims = sims / torch.tensor((k.size(self.col_dim))**0.5)\n",
    "\n",
    "        # Mask the scaled similarity scores of the later tokens so that the earlier tokens can't cheat during training\n",
    "        if mask is not None:\n",
    "            mask = mask.to(device) # move your mask to the target device because a manually created tensor lives in the cpu by default\n",
    "            scaled_sims = scaled_sims.masked_fill(mask=mask, value=-1e9)  # -1e9 is an approximation of negative infinity\n",
    "\n",
    "        attention_percents = F.softmax(scaled_sims, dim=self.col_dim)\n",
    "\n",
    "        # attention_scores are the contextualised embeddings\n",
    "        attention_scores = torch.matmul(attention_percents, v)\n",
    "\n",
    "        return attention_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Encoder-only Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderOnlyTransformer(L.LightningModule):\n",
    "\n",
    "    def __init__(self, num_tokens, d_model, max_len):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Word Embeddings\n",
    "        self.we = nn.Embedding(num_embeddings=num_tokens, embedding_dim=d_model)\n",
    "\n",
    "        # Position Encodings\n",
    "        self.pe = PositionEncoding(d_model=d_model, max_len=max_len)\n",
    "\n",
    "        # Self-Attention\n",
    "        self.attention = Attention(d_model=d_model)\n",
    "\n",
    "        # Classifier head\n",
    "        self.cls = nn.Linear(in_features=d_model, out_features=2)\n",
    "\n",
    "        # Calculate the loss with Cross Entropy; softmax is already included\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "\n",
    "        # Create word embeddings\n",
    "        word_embeddings = self.we(token_ids)\n",
    "\n",
    "        # Add position encodings to the word embeddings\n",
    "        position_encoded = self.pe(word_embeddings)\n",
    "\n",
    "        # Create Self-Attention layers\n",
    "        self_attention_values = self.attention(position_encoded,\n",
    "                                               position_encoded,\n",
    "                                               position_encoded,\n",
    "                                               mask=None) # no mask is needed\n",
    "        \n",
    "        # Add residual connections\n",
    "        residual_connection_values = position_encoded + self_attention_values\n",
    "    \n",
    "        # Pass the class token to the MLP\n",
    "        fc_layer_out = self.cls(residual_connection_values[0].unsqueeze(dim=0))\n",
    "    \n",
    "        return fc_layer_out\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return Adam(self.parameters(), lr=0.1)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_tokens, labels = batch\n",
    "        # print(f\"input shape: {input_tokens.shape}, labels shape: {labels.shape}\")\n",
    "        outputs = self.forward(input_tokens[0])\n",
    "        loss = self.loss(outputs, labels)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model predictions (logits) before training:\n",
      " tensor([[-0.0701, -0.9977]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "\n",
      "Model predictions (probabilities) before training:\n",
      " tensor([[0.7166, 0.2834]], device='mps:0', grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "model = EncoderOnlyTransformer(num_tokens=len(token_to_id), d_model=2, max_len=6).to(device) # move the model to mps\n",
    "\n",
    "predictions = model(inputs[0].to(device)) # the logits (scores) for each category\n",
    "print(f\"Model predictions (logits) before training:\\n {predictions}\")\n",
    "print()\n",
    "print(f\"Model predictions (probabilities) before training:\\n {torch.softmax(predictions, dim=1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model with `Lightning.Trainer.fit()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Create dataloader\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "dataset = TensorDataset(inputs, labels)\n",
    "dataloader = DataLoader(dataset)\n",
    "len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | we        | Embedding        | 16     | train\n",
      "1 | pe        | PositionEncoding | 0      | train\n",
      "2 | attention | Attention        | 12     | train\n",
      "3 | cls       | Linear           | 6      | train\n",
      "4 | loss      | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "34        Trainable params\n",
      "0         Non-trainable params\n",
      "34        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "/Users/edison/Git/pytorch-lightning-deep-learning/myenv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/Users/edison/Git/pytorch-lightning-deep-learning/myenv/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0429b1bddca4bd697f217a9595a75b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=30` reached.\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of trainer\n",
    "trainer = L.Trainer(max_epochs=30,\n",
    "                    accelerator=\"auto\")\n",
    "trainer.fit(model,\n",
    "            train_dataloaders=dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model after training with both training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device) # the model will be moved to cpu after calling trainer.fit() by default\n",
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<CLS>', 'machine', 'learning', 'is', 'fun', '<EOS>'], ['<CLS>', 'i', 'hate', 'machine', 'learning', '<EOS>']]\n"
     ]
    }
   ],
   "source": [
    "# Convert the token ids in the training samples to vocabs\n",
    "inputs_vocab = []\n",
    "for input in inputs:\n",
    "    input_vocab = list(map(lambda x: id_to_token[x.item()], input))\n",
    "    inputs_vocab.append(input_vocab)\n",
    "\n",
    "print(inputs_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sample 0: ['<CLS>', 'machine', 'learning', 'is', 'fun', '<EOS>'] | Prediction: 1\n",
      "Training sample 1: ['<CLS>', 'i', 'hate', 'machine', 'learning', '<EOS>'] | Prediction: 0\n"
     ]
    }
   ],
   "source": [
    "# Test the model after training with both training samples\n",
    "for input in inputs:\n",
    "    # Make a prediction with the model\n",
    "    pred = torch.argmax(model(input.to(device)), dim=1)\n",
    "\n",
    "for i, input in enumerate(inputs_vocab):\n",
    "    print(f\"Training sample {i}: {input} | Prediction: {torch.argmax(model(inputs[i].to(device)), dim=1).item()}\")    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
