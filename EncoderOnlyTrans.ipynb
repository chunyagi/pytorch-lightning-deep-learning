{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder-Only Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import lightning as L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from vocabs to numbers as nn.Embedding can only take integers\n",
    "token_to_id = {\"Edison\": 0,\n",
    "               \"is\": 1,\n",
    "               \"handsome\": 2,\n",
    "               \"but\": 3,\n",
    "               \"very\": 4,\n",
    "               \"lazy\": 5,\n",
    "               \"Danica\": 6,\n",
    "               \"loves\": 7,\n",
    "               \"doesn't\": 8,\n",
    "               \"work\": 9,\n",
    "               \"Bekzod\": 10,\n",
    "               \"<PAD>\": 11}\n",
    "\n",
    "# Create a mapping from numbers back to vocabs to interpret the output from the transformer\n",
    "id_to_token = dict(map(reversed, token_to_id.items()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training dataset\n",
    "# As the input is going to be word embeddings, we only need the corresponding numbers from the mapping\n",
    "inputs = torch.tensor([[token_to_id[\"Edison\"],\n",
    "                       token_to_id[\"is\"],\n",
    "                       token_to_id[\"handsome\"],\n",
    "                       token_to_id[\"<PAD>\"],\n",
    "                       token_to_id[\"<PAD>\"],\n",
    "                       token_to_id[\"<PAD>\"]], \n",
    "                       \n",
    "                      [token_to_id[\"Bekzod\"],\n",
    "                       token_to_id[\"very\"],\n",
    "                       token_to_id[\"lazy\"],\n",
    "                       token_to_id[\"doesn't\"],\n",
    "                       token_to_id[\"work\"],\n",
    "                       token_to_id[\"<PAD>\"]],\n",
    "                       \n",
    "                      [token_to_id[\"Edison\"],\n",
    "                       token_to_id[\"is\"],\n",
    "                       token_to_id[\"lazy\"],\n",
    "                       token_to_id[\"but\"],\n",
    "                       token_to_id[\"handsome\"],\n",
    "                       token_to_id[\"<PAD>\"]],\n",
    "                        \n",
    "                      [token_to_id[\"Edison\"],\n",
    "                       token_to_id[\"loves\"],\n",
    "                       token_to_id[\"work\"],\n",
    "                       token_to_id[\"but\"],\n",
    "                       token_to_id[\"lazy\"],\n",
    "                       token_to_id[\"<PAD>\"]],\n",
    "                       \n",
    "                      [token_to_id[\"Edison\"],\n",
    "                       token_to_id[\"is\"],\n",
    "                       token_to_id[\"lazy\"],\n",
    "                       token_to_id[\"<PAD>\"],\n",
    "                       token_to_id[\"<PAD>\"],\n",
    "                       token_to_id[\"<PAD>\"]],\n",
    "                        \n",
    "                      [token_to_id[\"but\"],\n",
    "                       token_to_id[\"Bekzod\"],\n",
    "                       token_to_id[\"doesn't\"],\n",
    "                       token_to_id[\"work\"],\n",
    "                       token_to_id[\"<PAD>\"],\n",
    "                       token_to_id[\"<PAD>\"]],\n",
    "                       \n",
    "                      [token_to_id[\"Danica\"],\n",
    "                       token_to_id[\"doesn't\"],\n",
    "                       token_to_id[\"loves\"],\n",
    "                       token_to_id[\"<PAD>\"],\n",
    "                       token_to_id[\"<PAD>\"],\n",
    "                       token_to_id[\"<PAD>\"]]])\n",
    "\n",
    "# 0-Negative; 1-Positive\n",
    "labels = torch.tensor([1, 0, 1, 0, 0, 0, 0])\n",
    "\n",
    "dataset = TensorDataset(inputs, labels)\n",
    "dataloader = DataLoader(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Position Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formula for the (standard, used in the paper **Attention is all you need**) position encoding is:  \n",
    "PE_(pos, 2i) = sin(pos / 10000^(2i / d_model))  \n",
    "PE_(pos, 2i+1) = cos(pos / 10000^(2i / d_model))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model=2, max_len=6):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # pe stands for position encoding\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "\n",
    "        # position is a column matrix (2D) of size [max_len, 1], e.g. [[0.], [1.], [2.]]\n",
    "        position = torch.arange(start=0, end=max_len, step=1).float().unsqueeze(1)\n",
    "\n",
    "        # Step is set to 2 because of \"2i\" in the formula, note that it is a 1D tensor, e.g. [0., 2.] as each position can have multiple embedding values\n",
    "        embedding_index = torch.arange(start=0, end=d_model, step=2).float()\n",
    "\n",
    "        # div_term is a row matrix (1D) with the same size as embedding_index\n",
    "        div_term = torch.tensor(10000.)**(embedding_index / d_model)\n",
    "\n",
    "        # Note: calculating the sin and cos values in this way only works when d_model is an even number, if d_model is odd, there will be a shape mismatch\n",
    "        pe[:, 0::2] = torch.sin(position / div_term)\n",
    "        pe[:, 1::2] = torch.cos(position / div_term)\n",
    "\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "\n",
    "    def forward(self, word_embeddings):\n",
    "\n",
    "        # Note: we might not need all the position encodings, as the number of tokens might not hit the maximum length (max_len)\n",
    "        return word_embeddings + self.pe[:word_embeddings.size(0), :]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model=2):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Create the weights associated with the query, key and value values\n",
    "        self.W_q = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        self.W_k = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        self.W_v = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "\n",
    "        self.row_dim = 0\n",
    "        self.col_dim = 1\n",
    "\n",
    "    def forward(self, encodings_for_q, encodings_for_k, encodings_for_v, mask=None):\n",
    "\n",
    "        # Create the Q, K and V matrices\n",
    "        q = self.W_q(encodings_for_q)\n",
    "        k = self.W_k(encodings_for_k)\n",
    "        v = self.W_v(encodings_for_v)\n",
    "\n",
    "        # Calculate the similarity score between the query values and key values\n",
    "        sims = torch.matmul(q, k.transpose(dim0=self.row_dim, dim1=self.col_dim))\n",
    "\n",
    "        # Scale the similarity score with the square root of d_model\n",
    "        scaled_sims = sims / torch.tensor((k.size(self.col_dim))**0.5)\n",
    "\n",
    "        device = scaled_sims.device\n",
    "        \n",
    "        # Mask the scaled similarity scores of the later tokens so that the earlier tokens can't cheat. Note: -1e9 is an approximation of negative infinity\n",
    "        if mask is not None:\n",
    "            # Move your mask to mps:0, or mask would live in cpu by default\n",
    "            mask = mask.to(device)\n",
    "            scaled_sims = scaled_sims.masked_fill(mask=mask, value=-1e9)\n",
    "\n",
    "        # Applying the softmax function to the scaled similarites determines the percentages of influence each token (in columns) should have on the others (in rows)\n",
    "        attention_percents = F.softmax(scaled_sims, dim=self.col_dim)\n",
    "\n",
    "        # attention_scores are basically the contextualised embeddings\n",
    "        attention_scores = torch.matmul(attention_percents, v)\n",
    "\n",
    "        return attention_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model=2, head=2):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Number of heads\n",
    "        self.head = head\n",
    "\n",
    "        self.attention = Attention(d_model=d_model)\n",
    "\n",
    "        # Linear layer to aggregate the attention scores from different heads\n",
    "        self.fc = nn.Linear(in_features=head*d_model, out_features=d_model)\n",
    "        \n",
    "    def forward(self, encodings_for_q, encodings_for_k, encodings_for_v, mask=None):\n",
    "\n",
    "        # List to store each head's attention values\n",
    "        agg_attention_values = []\n",
    "        \n",
    "        # Create attention heads\n",
    "        for h in range(self.head):\n",
    "\n",
    "            attention_values_h = self.attention(encodings_for_q,\n",
    "                                                encodings_for_k,\n",
    "                                                encodings_for_v,\n",
    "                                                mask=mask)\n",
    "\n",
    "            agg_attention_values.append(attention_values_h)\n",
    "\n",
    "        # Concatenate them as an input to the linear layer\n",
    "        agg_attention_values = torch.cat(agg_attention_values, dim=1)\n",
    "\n",
    "        # Run them through a linear layer\n",
    "        # Note: this is not the traditional approch of how multi-head attention\n",
    "        # print(\"the size of agg_attention_values is: \", agg_attention_values.size())\n",
    "        fc_layer_out = self.fc(agg_attention_values)\n",
    "\n",
    "        return fc_layer_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder-only Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderOnlyTransformer(L.LightningModule):\n",
    "\n",
    "    def __init__(self, num_tokens, d_model, head, max_len):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Word Embeddings\n",
    "        self.we = nn.Embedding(num_embeddings=num_tokens, embedding_dim=d_model)\n",
    "\n",
    "        # Position Encodings\n",
    "        self.pe = PositionEncoding(d_model=d_model, max_len=max_len)\n",
    "\n",
    "        # Multi-Head Attention\n",
    "        self.multi_head_attention = MultiHeadAttention(d_model=d_model, head=head)\n",
    "\n",
    "        # Classification head, out_features=2 because it's a binary classification task\n",
    "        self.cls = nn.Linear(in_features=max_len*d_model, out_features=2)\n",
    "\n",
    "        # Calculate the loss with Cross Entropy; softmax is already included\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    # The size of token_ids just needs to be a 1D tensor (without batching), unlike nn.LSTM, which requires the size of the input tensor to be [seq_len, batch_size, input_size]\n",
    "    def forward(self, token_ids):\n",
    "\n",
    "        # Create word embeddings\n",
    "        word_embeddings = self.we(token_ids)\n",
    "\n",
    "        # Add position encodings to the word embeddings\n",
    "        position_encoded = self.pe(word_embeddings)\n",
    "\n",
    "        # Multi-head Attention\n",
    "        self_attention_values = self.multi_head_attention(position_encoded,\n",
    "                                                          position_encoded,\n",
    "                                                          position_encoded,\n",
    "                                                          mask=None)\n",
    "        \n",
    "        # Add residual connections\n",
    "        # The shape of the residual_connection_values is [max_len, d_model]\n",
    "        residual_connection_values = position_encoded + self_attention_values\n",
    "\n",
    "        # Note: We need to concatenate the contextualised embeddings of the tokens in the residual connection values\n",
    "        # before passing it into the linear head\n",
    "        cls_input = residual_connection_values.flatten()\n",
    "        # It will return the logits\n",
    "        fc_layer_out = self.cls(cls_input)\n",
    "\n",
    "        return fc_layer_out\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return Adam(self.parameters(), lr=0.1)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \n",
    "        # input_tokens is a 2D tensor of size [batch_size, seq_len]\n",
    "        input_tokens, labels = batch\n",
    "        # print(batch, '\\n')\n",
    "        # print(input_tokens[0])\n",
    "        # outputs is fc_layer_out, so they share the same size\n",
    "        outputs = self.forward(input_tokens[0])\n",
    "        # Cross Entropy loss will automatically apply softmax to the outputs\n",
    "        loss = self.loss(outputs, labels[0])\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n"
     ]
    }
   ],
   "source": [
    "# Before we train the model, let's see what the model outputs for fun\n",
    "model = EncoderOnlyTransformer(num_tokens=len(token_to_id), d_model=2, head=2, max_len=6)\n",
    "\n",
    "model_input = torch.tensor([token_to_id[\"Bekzod\"],\n",
    "                            token_to_id[\"doesn't\"],\n",
    "                            token_to_id[\"work\"],\n",
    "                            token_to_id[\"<PAD>\"],\n",
    "                            token_to_id[\"<PAD>\"],\n",
    "                            token_to_id[\"<PAD>\"]])\n",
    "\n",
    "input_length = model_input.size(dim=0)\n",
    "\n",
    "# predictions is a 1D tensor of size [2] that contains the raw scores (logits) of the two classes\n",
    "predictions = model(model_input)\n",
    "# print(predictions)\n",
    "# Need to do argmax explicitly since the cross entropy loss is not used in a normal forward pass\n",
    "predicted_label = torch.argmax(predictions)\n",
    "print(predicted_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name                 | Type               | Params | Mode \n",
      "--------------------------------------------------------------------\n",
      "0 | we                   | Embedding          | 24     | train\n",
      "1 | pe                   | PositionEncoding   | 0      | train\n",
      "2 | multi_head_attention | MultiHeadAttention | 22     | train\n",
      "3 | cls                  | Linear             | 26     | train\n",
      "4 | loss                 | CrossEntropyLoss   | 0      | train\n",
      "--------------------------------------------------------------------\n",
      "72        Trainable params\n",
      "0         Non-trainable params\n",
      "72        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "/Users/edison/Git/pytorch-playground/myenv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/Users/edison/Git/pytorch-playground/myenv/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c0b3408f9a9456db0032f52669754ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    }
   ],
   "source": [
    "# That means we need to train the model...\n",
    "trainer = L.Trainer(max_epochs=100)\n",
    "trainer.fit(model, train_dataloaders=dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "# Run the same code after training\n",
    "model_input = torch.tensor([token_to_id[\"Bekzod\"],\n",
    "                            token_to_id[\"doesn't\"],\n",
    "                            token_to_id[\"work\"],\n",
    "                            token_to_id[\"<PAD>\"],\n",
    "                            token_to_id[\"<PAD>\"],\n",
    "                            token_to_id[\"<PAD>\"]])\n",
    "\n",
    "input_length = model_input.size(dim=0)\n",
    "\n",
    "# predictions is a 1D tensor of size [2] that contains the raw scores (logits) of the two classes\n",
    "predictions = model(model_input)\n",
    "# print(predictions)\n",
    "# Need to do argmax explicitly since the cross entropy loss is not used in a normal forward pass\n",
    "predicted_label = torch.argmax(predictions)\n",
    "print(predicted_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Edison', 'is', 'handsome', '<PAD>', '<PAD>', '<PAD>']: 1\n",
      "\n",
      "['Bekzod', 'very', 'lazy', \"doesn't\", 'work', '<PAD>']: 0\n",
      "\n",
      "['Edison', 'is', 'lazy', 'but', 'handsome', '<PAD>']: 1\n",
      "\n",
      "['Edison', 'loves', 'work', 'but', 'lazy', '<PAD>']: 0\n",
      "\n",
      "['Edison', 'is', 'lazy', '<PAD>', '<PAD>', '<PAD>']: 0\n",
      "\n",
      "['but', 'Bekzod', \"doesn't\", 'work', '<PAD>', '<PAD>']: 0\n",
      "\n",
      "['Danica', \"doesn't\", 'loves', '<PAD>', '<PAD>', '<PAD>']: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the model with the entire training dataset\n",
    "predicted_labels = []\n",
    "\n",
    "for input in inputs:\n",
    "    predicted_label = torch.argmax(model(input))\n",
    "    predicted_labels.append(predicted_label.item())\n",
    "\n",
    "for pair in zip(inputs, predicted_labels):\n",
    "    input_id, label = pair\n",
    "    input_token = list(map(lambda id: id_to_token[id.item()], input_id))\n",
    "    print(f\"{input_token}: {label}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "# Test it with a sentence that is not included in the training dataset\n",
    "model_input = torch.tensor([token_to_id[\"Danica\"],\n",
    "                            token_to_id[\"loves\"],\n",
    "                            token_to_id[\"Edison\"],\n",
    "                            token_to_id[\"but\"],\n",
    "                            token_to_id[\"doesn't\"],\n",
    "                            token_to_id[\"work\"]])\n",
    "\n",
    "input_length = model_input.size(dim=0)\n",
    "\n",
    "# predictions is a 1D tensor of size [2] that contains the raw scores (logits) of the two classes\n",
    "predictions = model(model_input)\n",
    "# print(predictions)\n",
    "# Need to do argmax explicitly since the cross entropy loss is not used in a normal forward pass\n",
    "predicted_label = torch.argmax(predictions)\n",
    "print(predicted_label)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
