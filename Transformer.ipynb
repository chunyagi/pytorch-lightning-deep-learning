{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replicate the Transformer architecture with PyTorch + Lightning\n",
    "\n",
    "In this notebook, we're going to replicate the Transformer architecture in the paper [*Attention Is All You Need*](https://arxiv.org/abs/1706.03762) with PyTorch and Lightning. The goal is to train a transformer model to translate english sentences to spanish sentences (**machine translation**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import lightning as L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup device-agnostic code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create tokenizers\n",
    "\n",
    "We need to create 2 separate tokenizers for both the source language (English in our case) and the target language (Spanish)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({0: '<SOS>',\n",
       "  1: 'i',\n",
       "  2: 'love',\n",
       "  3: 'you',\n",
       "  4: 'me',\n",
       "  5: 'see',\n",
       "  6: 'like',\n",
       "  7: 'pizza',\n",
       "  8: 'cake',\n",
       "  9: 'eat',\n",
       "  10: '<EOS>',\n",
       "  11: '<PAD>'},\n",
       " {0: '<SOS>',\n",
       "  1: 'te',\n",
       "  2: 'amo',\n",
       "  3: 'me',\n",
       "  4: 'amas',\n",
       "  5: 'veo',\n",
       "  6: 'ves',\n",
       "  7: 'gusta',\n",
       "  8: 'pizza',\n",
       "  9: 'pastel',\n",
       "  10: 'yo',\n",
       "  11: 'como',\n",
       "  12: 'tú',\n",
       "  13: 'comes',\n",
       "  14: '<EOS>',\n",
       "  15: '<PAD>'})"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a tokenizer for the source language (English)\n",
    "token_to_id_src = {\n",
    "    \"<SOS>\": 0,\n",
    "    \"i\": 1,\n",
    "    \"love\": 2,\n",
    "    \"you\": 3,\n",
    "    \"me\": 4,\n",
    "    \"see\": 5,\n",
    "    \"like\": 6,\n",
    "    \"pizza\": 7,\n",
    "    \"cake\": 8,\n",
    "    \"eat\": 9,\n",
    "    \"<EOS>\": 10,\n",
    "    \"<PAD>\": 11\n",
    "}\n",
    "\n",
    "# Create a tokenizer for the target language (Spanish)\n",
    "token_to_id_target = {\n",
    "    \"<SOS>\": 0,\n",
    "    \"te\": 1,\n",
    "    \"amo\": 2,\n",
    "    \"me\": 3,\n",
    "    \"amas\": 4,\n",
    "    \"veo\": 5,\n",
    "    \"ves\": 6,\n",
    "    \"gusta\": 7,\n",
    "    \"pizza\": 8,\n",
    "    \"pastel\": 9,\n",
    "    \"yo\": 10,\n",
    "    \"como\": 11,\n",
    "    \"tú\": 12,\n",
    "    \"comes\": 13,\n",
    "    \"<EOS>\": 14,\n",
    "    \"<PAD>\": 15\n",
    "}\n",
    "\n",
    "# Create a mapping from tokens to numbers to interpret the output from the transformer later\n",
    "id_to_token_src = dict(map(reversed, token_to_id_src.items())) # not necessary here\n",
    "id_to_token_target = dict(map(reversed, token_to_id_target.items()))\n",
    "id_to_token_src, id_to_token_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0,  1,  2,  3, 10],\n",
       "         [ 0,  3,  2,  4, 10],\n",
       "         [ 0,  1,  5,  3, 10],\n",
       "         [ 0,  3,  5,  4, 10],\n",
       "         [ 0,  1,  6,  7, 10],\n",
       "         [ 0,  3,  6,  8, 10],\n",
       "         [ 0,  1,  9,  7, 10],\n",
       "         [ 0,  3,  9,  8, 10]]),\n",
       " tensor([[ 0,  1,  2, 14, 15],\n",
       "         [ 0,  3,  4, 14, 15],\n",
       "         [ 0,  1,  5, 14, 15],\n",
       "         [ 0,  3,  6, 14, 15],\n",
       "         [ 0,  3,  7,  8, 14],\n",
       "         [ 0,  1,  7,  9, 14],\n",
       "         [ 0, 10, 11,  8, 14],\n",
       "         [ 0, 12, 13,  9, 14]]))"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = torch.tensor([\n",
    "    [token_to_id_src[\"<SOS>\"], token_to_id_src[\"i\"],   token_to_id_src[\"love\"], token_to_id_src[\"you\"],   token_to_id_src[\"<EOS>\"]],\n",
    "    [token_to_id_src[\"<SOS>\"], token_to_id_src[\"you\"], token_to_id_src[\"love\"], token_to_id_src[\"me\"],    token_to_id_src[\"<EOS>\"]],\n",
    "    [token_to_id_src[\"<SOS>\"], token_to_id_src[\"i\"],   token_to_id_src[\"see\"],  token_to_id_src[\"you\"],   token_to_id_src[\"<EOS>\"]],\n",
    "    [token_to_id_src[\"<SOS>\"], token_to_id_src[\"you\"], token_to_id_src[\"see\"],  token_to_id_src[\"me\"],    token_to_id_src[\"<EOS>\"]],\n",
    "    [token_to_id_src[\"<SOS>\"], token_to_id_src[\"i\"],   token_to_id_src[\"like\"], token_to_id_src[\"pizza\"], token_to_id_src[\"<EOS>\"]],\n",
    "    [token_to_id_src[\"<SOS>\"], token_to_id_src[\"you\"], token_to_id_src[\"like\"], token_to_id_src[\"cake\"],  token_to_id_src[\"<EOS>\"]],\n",
    "    [token_to_id_src[\"<SOS>\"], token_to_id_src[\"i\"],   token_to_id_src[\"eat\"],  token_to_id_src[\"pizza\"], token_to_id_src[\"<EOS>\"]],\n",
    "    [token_to_id_src[\"<SOS>\"], token_to_id_src[\"you\"], token_to_id_src[\"eat\"],  token_to_id_src[\"cake\"],  token_to_id_src[\"<EOS>\"]],\n",
    "])\n",
    "\n",
    "labels = torch.tensor([\n",
    "    [token_to_id_target[\"<SOS>\"], token_to_id_target[\"te\"],  token_to_id_target[\"amo\"],   token_to_id_target[\"<EOS>\"], token_to_id_target[\"<PAD>\"]],\n",
    "    [token_to_id_target[\"<SOS>\"], token_to_id_target[\"me\"],  token_to_id_target[\"amas\"],  token_to_id_target[\"<EOS>\"], token_to_id_target[\"<PAD>\"]],\n",
    "    [token_to_id_target[\"<SOS>\"], token_to_id_target[\"te\"],  token_to_id_target[\"veo\"],   token_to_id_target[\"<EOS>\"], token_to_id_target[\"<PAD>\"]],\n",
    "    [token_to_id_target[\"<SOS>\"], token_to_id_target[\"me\"],  token_to_id_target[\"ves\"],   token_to_id_target[\"<EOS>\"], token_to_id_target[\"<PAD>\"]],\n",
    "    [token_to_id_target[\"<SOS>\"], token_to_id_target[\"me\"],  token_to_id_target[\"gusta\"], token_to_id_target[\"pizza\"],  token_to_id_target[\"<EOS>\"]],\n",
    "    [token_to_id_target[\"<SOS>\"], token_to_id_target[\"te\"],  token_to_id_target[\"gusta\"], token_to_id_target[\"pastel\"], token_to_id_target[\"<EOS>\"]],\n",
    "    [token_to_id_target[\"<SOS>\"], token_to_id_target[\"yo\"],  token_to_id_target[\"como\"],  token_to_id_target[\"pizza\"],  token_to_id_target[\"<EOS>\"]],\n",
    "    [token_to_id_target[\"<SOS>\"], token_to_id_target[\"tú\"],  token_to_id_target[\"comes\"], token_to_id_target[\"pastel\"], token_to_id_target[\"<EOS>\"]],\n",
    "])\n",
    "inputs, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review of the transformer architecture\n",
    "\n",
    "Let's break down the transformer architecture in smaller components:\n",
    "1. Tokenizer: map the text to numbers.\n",
    "2. Embedding layer: map the tokens to embeddings.\n",
    "3. Positional encoding: encode the positional information of the tokens\n",
    "4. Multi-Head Attention block: the core part of the transformer architecture, it computes the attention values for each token (how much should each token attend to the others in the sequence, including itself). Note: For simplicity, we'll only do self-attention (one single head) in this notebook.\n",
    "    Different types of attention mechanisms are used in the paper:\n",
    "    - (Standard) Self-Attention: Bidirectional, used in the encoder\n",
    "    - Masked Self-Attention: Uni-directional where each token can only attend to itself and preceding tokens (future tokens are masked). It's used in the decoder to prevent information leakage from future tokens during training\n",
    "    - Encoder-Decoder Attention: Bidirectional self-attention where the tokens in the decoder attends to any position of the encoder's output tokens (contextualised embeddings). It's used in the decoder to further enrich the token embeddings' context information from the embeddings output by the encoder after masked self-attention\n",
    "Every time after going through a self-attention layer, we need to add the residual connections and then normalise the sum with a layer norm layer.\n",
    "5. MLP block: fully-connected layers coupled with an \"add & norm\" layer\n",
    "\n",
    "### Figure 1\n",
    "Figure 1 visualizes the model architecture of the transformer used in the paper:</br>\n",
    "<img src=\"transformer_architecture.png\" width=300 alt=\"figure 1 from transformer paper\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create position encoding\n",
    "\n",
    "We will use the formula used in the paper *Attention is all you need* position encoding is:\n",
    "* PE_(pos, 2i) = sin(pos / 10000^(2i / d_model))\n",
    "* PE_(pos, 2i+1) = cos(pos / 10000^(2i / d_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model=6, max_len=6):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "\n",
    "        position = torch.arange(start=0, end=max_len, step=1).float().unsqueeze(1)\n",
    "\n",
    "        embedding_index = torch.arange(start=0, end=d_model, step=2).float()\n",
    "\n",
    "        div_term = torch.tensor(10000)**(embedding_index / d_model)\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position / div_term)\n",
    "        pe[:, 1::2] = torch.cos(position / div_term)\n",
    "\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "    \n",
    "    def forward(self, word_embeddings):\n",
    "\n",
    "        return word_embeddings + self.pe[:word_embeddings.size(0), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Attention layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 d_model=2,\n",
    "                 mask: torch.tensor = None):\n",
    "                         \n",
    "        super().__init__()\n",
    "\n",
    "        self.W_q = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        self.W_k = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        self.W_v = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "\n",
    "        self.register_buffer(name=\"mask\",\n",
    "                             tensor=mask)\n",
    "\n",
    "        self.row_dim = 0\n",
    "        self.col_dim = 1\n",
    "\n",
    "    def forward(self, encodings_for_q, encodings_for_k, encodings_for_v):\n",
    "\n",
    "        # Create the Q, K and V matrices\n",
    "        q = self.W_q(encodings_for_q)\n",
    "        k = self.W_k(encodings_for_k)\n",
    "        v = self.W_v(encodings_for_v)\n",
    "\n",
    "        # Calculate the similarity score between the queries and values\n",
    "        sims = torch.matmul(q, k.transpose(dim0=self.row_dim, dim1=self.col_dim))\n",
    "\n",
    "        # Scale the similarity score with the square root of d_model\n",
    "        scaled_sims = sims / torch.tensor((k.size(self.col_dim))**0.5)\n",
    "\n",
    "        # Mask the scaled similarity scores of the later tokens so that the earlier tokens can't cheat during training\n",
    "        if self.mask is not None:\n",
    "            scaled_sims = scaled_sims.masked_fill(mask=self.mask, \n",
    "                                                  value=-1e9)  # -1e9 is an approximation of negative infinity\n",
    "\n",
    "        attention_percents = F.softmax(scaled_sims, dim=self.col_dim)\n",
    "\n",
    "        # attention_scores are the contextualised embeddings\n",
    "        attention_scores = torch.matmul(attention_percents, v)\n",
    "\n",
    "        return attention_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Encoder\n",
    "\n",
    "Workflow:\n",
    "1. Tokenize the source text\n",
    "2. Pass the tokens through the embedding layer\n",
    "3. Add positional encodings\n",
    "4. Pass the embeddings through the attention layer\n",
    "5. Pass the contextualised embeddings through the mlp block\n",
    "6. Return the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Replicating the encoder block in the transformer paper. It returns the contextualised embeddings of the source text tokens.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 num_tokens: int, \n",
    "                 d_model: int = 6,\n",
    "                 max_len: int = 5):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Word Embeddings\n",
    "        self.we = nn.Embedding(num_embeddings=num_tokens, embedding_dim=d_model)\n",
    "\n",
    "        # Position Encodings\n",
    "        self.pe = PositionEncoding(d_model=d_model, max_len=max_len)\n",
    "\n",
    "        # Attention block\n",
    "        self.attention = Attention(d_model=d_model,\n",
    "                                   mask=None)\n",
    "\n",
    "        # Layer norm 1\n",
    "        self.layer_norm1 = nn.LayerNorm(normalized_shape=d_model)\n",
    "\n",
    "        # Layer norm 2\n",
    "        self.layer_norm2 = nn.LayerNorm(normalized_shape=d_model)\n",
    "\n",
    "        # Linear layer\n",
    "        self.linear_layer = nn.Linear(in_features=d_model,\n",
    "                                      out_features=d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Create word embeddings\n",
    "        word_embeddings = self.we(x)\n",
    "\n",
    "        # Add position encodings to the word embeddings\n",
    "        position_encoded = self.pe(word_embeddings)\n",
    "\n",
    "        # Run the positional encoded embeddings through a self-attention layer\n",
    "        attention_values = self.attention(position_encoded,\n",
    "                                          position_encoded,\n",
    "                                          position_encoded)\n",
    "        \n",
    "        # Add residual connections and then apply layer norm\n",
    "        normalized_attention_values = self.layer_norm1(attention_values + word_embeddings)\n",
    "\n",
    "        # Run the normalised attention values through a mlp block (linear layer + add & norm layer)\n",
    "        return self.layer_norm2(self.linear_layer(normalized_attention_values) + normalized_attention_values) # operation fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.4281, -1.3818, -0.9599,  1.0252, -0.2459,  0.1343],\n",
       "        [ 0.4991,  0.1097, -0.8558, -0.4280, -1.1783,  1.8533],\n",
       "        [-0.7663,  0.0183, -0.1691,  1.6283,  0.7578, -1.4691],\n",
       "        [ 0.3784, -1.6927, -0.2539, -0.3325,  0.2397,  1.6610],\n",
       "        [-0.1332,  1.7643, -0.4780,  0.3334,  0.1007, -1.5873]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the encoder\n",
    "encoder = EncoderBlock(num_tokens=len(token_to_id_src))\n",
    "\n",
    "# Forward pass\n",
    "encoder_output = encoder(inputs[0])\n",
    "encoder_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Decoder\n",
    "\n",
    "Workflow:\n",
    "1. Tokenize the target text\n",
    "2. Pass the tokens through the embedding layer\n",
    "3. Add positional encodings\n",
    "4. Pass them through the masked attention layer\n",
    "5. Pass them through the mlp block\n",
    "6. Pass them through the encoder-decoder attention layer\n",
    "7. Pass them through another mlp block\n",
    "8. Return the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Replicating the decoder block in the transformer paper. It returns the contextualised embeddings of the target text tokens.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 num_tokens: int,\n",
    "                 d_model: int = 6, \n",
    "                 max_len: int = 5):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        # Word Embeddings\n",
    "        self.we = nn.Embedding(num_embeddings=num_tokens, embedding_dim=d_model)\n",
    "\n",
    "        # Positional Encodings\n",
    "        self.pe = PositionEncoding(d_model=d_model, max_len=max_len)\n",
    "\n",
    "        # Masked Attention layer\n",
    "        self.decoder_attention = Attention(d_model=d_model,\n",
    "                                           mask=(torch.tril(torch.ones(max_len, max_len)) == 0)) # the shape of mask is: [seq_len, seq_len])\n",
    "        \n",
    "        # Layer Norm 1\n",
    "        self.layer_norm1 = nn.LayerNorm(normalized_shape=d_model)\n",
    "        \n",
    "        # Layer Norm 2\n",
    "        self.layer_norm2 = nn.LayerNorm(normalized_shape=d_model)\n",
    "        \n",
    "        # Layer Norm 3\n",
    "        self.layer_norm3 = nn.LayerNorm(normalized_shape=d_model)\n",
    "        \n",
    "        # Encoder-Decoder Attention layer\n",
    "        self.encoder_decoder_attention = Attention(d_model=d_model,\n",
    "                                                   mask=None)\n",
    "        # Linear layer\n",
    "        self.linear_layer = nn.Linear(in_features=d_model,\n",
    "                                      out_features=d_model)\n",
    "\n",
    "    def forward(self, \n",
    "                x,\n",
    "                encoder_embeddings):\n",
    "\n",
    "        # Create word embeddings\n",
    "        word_embeddings = self.we(x)\n",
    "\n",
    "        # Add positional encodings to the word embeddings\n",
    "        decoder_positional_embeddings = self.pe(word_embeddings)\n",
    "\n",
    "        # Run the positionl encoded embeddings through the masked attention layer\n",
    "        decoder_attention_values = self.decoder_attention(decoder_positional_embeddings,\n",
    "                                                          decoder_positional_embeddings,\n",
    "                                                          decoder_positional_embeddings)\n",
    "\n",
    "        # Add & Norm layer 1\n",
    "        normalized_decoder_attention_values = self.layer_norm1(decoder_attention_values + word_embeddings)\n",
    "\n",
    "        # Cross attention layer\n",
    "        cross_attention_values = self.encoder_decoder_attention(normalized_decoder_attention_values,\n",
    "                                                                encoder_embeddings,\n",
    "                                                                encoder_embeddings)\n",
    "        # Add & Norm layer 2\n",
    "        normalized_cross_attention_values = self.layer_norm2(cross_attention_values + normalized_decoder_attention_values)\n",
    "\n",
    "        # MLP block\n",
    "        return self.layer_norm3(self.linear_layer(normalized_cross_attention_values) + normalized_cross_attention_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2, 14, 15])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1319,  0.7287,  1.5256, -0.5534, -1.1188,  0.5498],\n",
       "        [ 0.7780, -1.1195, -1.0119,  1.7358, -0.0662, -0.3164],\n",
       "        [ 0.5016, -1.1174, -1.4974,  0.3349,  1.4203,  0.3578],\n",
       "        [ 1.7898, -1.1354, -0.4221,  0.1237,  0.6128, -0.9688],\n",
       "        [-2.0327,  0.1249, -0.1078,  0.7596,  1.1153,  0.1408]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the decoder\n",
    "decoder = DecoderBlock(num_tokens=len(token_to_id_target))\n",
    "decoder(labels[0],\n",
    "        encoder_embeddings=encoder_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Transformer\n",
    "\n",
    "Now it's time to put all the puzzles together.\n",
    "\n",
    "Workflow:\n",
    "1. Pass the tokenized sequence in source language through the encoder block\n",
    "2. Pass the tokenized sequence in target language through the decoder block\n",
    "3. Pass the result from the decoder through the last linear layer\n",
    "4. Run the resullt through a softmax layer\n",
    "5. Return the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(L.LightningModule):\n",
    "    \"\"\"Replicate the transformer architecture from the paper.\"\"\"\n",
    "    def __init__(self,\n",
    "                 num_tokens_src: int, # number of vocabs in the source language\n",
    "                 num_tokens_target: int, # number of vocabs in the target language\n",
    "                 d_model: int = 6,\n",
    "                 max_len: int = 5):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        # Create an encoder block\n",
    "        self.encoder_block = EncoderBlock(\n",
    "            num_tokens=num_tokens_src,\n",
    "            d_model=d_model,\n",
    "            max_len=max_len\n",
    "        )\n",
    "\n",
    "        # Create a decoder block\n",
    "        self.decoder_block = DecoderBlock(\n",
    "            num_tokens=num_tokens_target,\n",
    "            d_model=d_model,\n",
    "            max_len=max_len\n",
    "        )\n",
    "\n",
    "        # Create the linear layer\n",
    "        self.linear_layer = nn.Linear(in_features=d_model,\n",
    "                                      out_features=num_tokens_target) # how many vocabs are there in the target language tokenizer\n",
    "        \n",
    "        # Setup loss function\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, x_src, x_target): # the target sequence is also needed for teacher forcing during training\n",
    "        \n",
    "        # Pass the source sequence to the encoder block\n",
    "        encoder_embeddings = self.encoder_block(x_src)\n",
    "\n",
    "        # Pass the target sequence to the decoder block\n",
    "        decoder_embeddings = self.decoder_block(x=x_target,\n",
    "                                                encoder_embeddings=encoder_embeddings)\n",
    "        \n",
    "        # Pass the result through the final linear layer\n",
    "        return self.linear_layer(decoder_embeddings)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return Adam(self.parameters(), lr=0.1)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_tokens, labels = batch\n",
    "        outputs = self.forward(x_src=input_tokens[0],\n",
    "                               x_target=labels[0])\n",
    "        loss = self.loss_fn(outputs, labels[0])\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2152, -0.4546,  0.0394,  0.0677, -0.0545,  0.8017, -0.0953,  0.4340,\n",
       "         -0.6535,  0.0716,  0.5408,  0.2377, -0.7780, -0.2315,  0.6845, -0.2330],\n",
       "        [ 0.1591, -0.4489, -0.5699, -0.5113, -0.5825, -0.1202,  0.1925,  0.0018,\n",
       "          0.9156,  0.6455,  0.7619, -0.7661, -0.0649,  0.4199,  0.2110, -0.2469],\n",
       "        [ 0.1039, -0.5878,  1.1718, -0.1426,  0.8702,  0.3124, -0.1313, -0.1212,\n",
       "          0.5813, -0.1304,  0.2518,  0.6831, -0.7163,  1.0081,  0.3152,  0.6601],\n",
       "        [-0.4179, -0.4270, -1.2409,  0.0746, -1.2202,  0.4409,  0.2123,  0.5776,\n",
       "         -0.4422,  0.5272,  0.9960, -0.7501, -0.2821, -0.2973,  0.4988, -0.6226],\n",
       "        [-0.2152, -0.1593, -1.1012, -0.3428,  0.2441,  0.1139,  0.6229, -0.3206,\n",
       "          0.6523, -0.0179,  0.8213, -0.7836, -0.0246,  0.6720,  0.9047,  0.5303]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the transformer\n",
    "model = Transformer(num_tokens_src=len(token_to_id_src),\n",
    "                    num_tokens_target=len(token_to_id_target))\n",
    "model(x_src=inputs[0],\n",
    "      x_target=labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model with `Lightning.Trainer.fit()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Create dataloader\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "dataset = TensorDataset(inputs, labels)\n",
    "dataloader = DataLoader(dataset)\n",
    "len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name          | Type             | Params | Mode \n",
      "-----------------------------------------------------------\n",
      "0 | encoder_block | EncoderBlock     | 246    | train\n",
      "1 | decoder_block | DecoderBlock     | 390    | train\n",
      "2 | linear_layer  | Linear           | 112    | train\n",
      "3 | loss_fn       | CrossEntropyLoss | 0      | train\n",
      "-----------------------------------------------------------\n",
      "748       Trainable params\n",
      "0         Non-trainable params\n",
      "748       Total params\n",
      "0.003     Total estimated model params size (MB)\n",
      "/Users/edison/Git/pytorch-lightning-deep-learning/myenv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/Users/edison/Git/pytorch-lightning-deep-learning/myenv/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (8) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1289b95cff7448f9e9ad80d3b856bda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=60` reached.\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of trainer\n",
    "trainer = L.Trainer(max_epochs=60,\n",
    "                    accelerator=\"auto\")\n",
    "trainer.fit(model,\n",
    "            train_dataloaders=dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: ['<SOS>', 'i', 'love', 'you', '<EOS>'] | Pred: ['<SOS>', 'te', 'amo', '<EOS>', '<PAD>']\n",
      "Input: ['<SOS>', 'you', 'love', 'me', '<EOS>'] | Pred: ['<SOS>', 'me', 'amas', '<EOS>', '<PAD>']\n",
      "Input: ['<SOS>', 'i', 'see', 'you', '<EOS>'] | Pred: ['<SOS>', 'te', 'veo', '<EOS>', '<PAD>']\n",
      "Input: ['<SOS>', 'you', 'see', 'me', '<EOS>'] | Pred: ['<SOS>', 'me', 'ves', '<EOS>', '<PAD>']\n",
      "Input: ['<SOS>', 'i', 'like', 'pizza', '<EOS>'] | Pred: ['<SOS>', 'me', 'gusta', 'pizza', '<EOS>']\n",
      "Input: ['<SOS>', 'you', 'like', 'cake', '<EOS>'] | Pred: ['<SOS>', 'te', 'gusta', 'pastel', '<EOS>']\n",
      "Input: ['<SOS>', 'i', 'eat', 'pizza', '<EOS>'] | Pred: ['<SOS>', 'yo', 'como', 'pizza', '<EOS>']\n",
      "Input: ['<SOS>', 'you', 'eat', 'cake', '<EOS>'] | Pred: ['<SOS>', 'tú', 'comes', 'pastel', '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "# Test the transformer\n",
    "for i in range(len(inputs)):\n",
    "      y_logits = model(x_src=inputs[i],\n",
    "                  x_target=labels[i])\n",
    "\n",
    "      pred = list(map(lambda x: id_to_token_target[x.item()], torch.argmax(y_logits, dim=1))) \n",
    "      print(f\"Input: {list(map(lambda x: id_to_token_src[x.item()], inputs[i]))} | Pred: {pred}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
