{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replicate the Transformer architecture with PyTorch + Lightning\n",
    "\n",
    "In this notebook, we're going to replicate the Transformer architecture in the paper [*Attention Is All You Need*](https://arxiv.org/abs/1706.03762) with PyTorch and Lightning. The goal is to train a transformer model to translate english sentences to spanish sentences (**machine translation**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import lightning as L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup device-agnostic code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create tokenizers\n",
    "\n",
    "We need to create 2 separate tokenizers for both the source language (English in our case) and the target language (Spanish)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({0: '<SOS>', 1: 'i', 2: 'love', 3: 'you', 4: 'me', 5: '<EOS>'},\n",
       " {0: '<SOS>', 1: 'te', 2: 'amo', 3: 'me', 4: 'amas', 5: '<EOS>'})"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a tokenizer for the source language (English)\n",
    "token_to_id_src = {\n",
    "    \"<SOS>\": 0,\n",
    "    \"i\": 1,\n",
    "    \"love\": 2,\n",
    "    \"you\": 3,\n",
    "    \"me\": 4,\n",
    "    \"<EOS>\": 5\n",
    "}\n",
    "\n",
    "# Create a tokenizer for the target language (Spanish)\n",
    "token_to_id_target = {\n",
    "    \"<SOS>\": 0, # tells the decoder to start generating tokens\n",
    "    \"te\": 1,\n",
    "    \"amo\": 2,\n",
    "    \"me\": 3,\n",
    "    \"amas\": 4,\n",
    "    \"<EOS>\": 5 # tells the decoder to stop generating tokens\n",
    "}\n",
    "\n",
    "# Create a mapping from tokens to numbers to interpret the output from the transformer later\n",
    "id_to_token_src = dict(map(reversed, token_to_id_src.items())) # not necessary here\n",
    "id_to_token_target = dict(map(reversed, token_to_id_target.items()))\n",
    "id_to_token_src, id_to_token_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 1, 2, 3, 5],\n",
       "         [0, 3, 2, 4, 5]]),\n",
       " tensor([[0, 1, 2, 5],\n",
       "         [0, 3, 4, 5]]))"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = torch.tensor([\n",
    "    [\n",
    "        token_to_id_src[\"<SOS>\"],\n",
    "        token_to_id_src[\"i\"],\n",
    "        token_to_id_src[\"love\"],\n",
    "        token_to_id_src[\"you\"],\n",
    "        token_to_id_src[\"<EOS>\"]\n",
    "    ],\n",
    "    \n",
    "    [\n",
    "        token_to_id_src[\"<SOS>\"],\n",
    "        token_to_id_src[\"you\"],\n",
    "        token_to_id_src[\"love\"],\n",
    "        token_to_id_src[\"me\"],\n",
    "        token_to_id_src[\"<EOS>\"],\n",
    "    ]\n",
    "])\n",
    "\n",
    "labels = torch.tensor([\n",
    "    [\n",
    "        token_to_id_target[\"<SOS>\"],\n",
    "        token_to_id_target[\"te\"],\n",
    "        token_to_id_target[\"amo\"],\n",
    "        token_to_id_target[\"<EOS>\"]\n",
    "    ],\n",
    "\n",
    "    [\n",
    "        token_to_id_target[\"<SOS>\"],\n",
    "        token_to_id_target[\"me\"],\n",
    "        token_to_id_target[\"amas\"],\n",
    "        token_to_id_target[\"<EOS>\"]\n",
    "    ]\n",
    "])\n",
    "inputs, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review of the transformer architecture\n",
    "\n",
    "Let's break down the transformer architecture in smaller components:\n",
    "1. Tokenizer: map the text to numbers.\n",
    "2. Embedding layer: map the tokens to embeddings.\n",
    "3. Positional Encoding: add the positional information to the embeddings.\n",
    "4. Multi-Head Attention block: the core part of the transformer architecture, it computes the attention values each token will give to every other token in the sequence (including itself). For simplicity, we'll only implement self-attention in this notebook. </br>\n",
    "    **Note:** There are two different types of attention used in the paper:\n",
    "    - (Standard) Self-Attention: Bidirectional, used by the encoder\n",
    "    - Masked Self-Attention: Unidirectional, tokens that come after the query token are masked, used by the decoder\n",
    "    - Encoder-Decoder Attention: the decoder can attend to any position to the encoder's output\n",
    "5. MLP block\n",
    "\n",
    "### Figure 1\n",
    "<img src=\"transformer_architecture.png\" width=300 alt=\"figure 1 from transformer paper\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create position encoding\n",
    "\n",
    "We will use the formula used in the paper *Attention is all you need* position encoding is:\n",
    "* PE_(pos, 2i) = sin(pos / 10000^(2i / d_model))\n",
    "* PE_(pos, 2i+1) = cos(pos / 10000^(2i / d_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model=6, max_len=6):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "\n",
    "        position = torch.arange(start=0, end=max_len, step=1).float().unsqueeze(1)\n",
    "\n",
    "        embedding_index = torch.arange(start=0, end=d_model, step=2).float()\n",
    "\n",
    "        div_term = torch.tensor(10000)**(embedding_index / d_model)\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position / div_term)\n",
    "        pe[:, 1::2] = torch.cos(position / div_term)\n",
    "\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "    \n",
    "    def forward(self, word_embeddings):\n",
    "\n",
    "        return word_embeddings + self.pe[:word_embeddings.size(0), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Attention layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model=6):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.W_q = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        self.W_k = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        self.W_v = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "\n",
    "        self.row_dim = 0\n",
    "        self.col_dim = 1\n",
    "\n",
    "    def forward(self, encodings_for_q, encodings_for_k, encodings_for_v, mask=None, device=device):\n",
    "\n",
    "        # Create the Q, K and V matrices\n",
    "        q = self.W_q(encodings_for_q)\n",
    "        k = self.W_k(encodings_for_k)\n",
    "        v = self.W_v(encodings_for_v)\n",
    "\n",
    "        # Calculate the similarity score between the queries and values\n",
    "        sims = torch.matmul(q, k.transpose(dim0=self.row_dim, dim1=self.col_dim))\n",
    "\n",
    "        # Scale the similarity score with the square root of d_model\n",
    "        scaled_sims = sims / torch.tensor((k.size(self.col_dim))**0.5)\n",
    "\n",
    "        # Mask the scaled similarity scores of the later tokens so that the earlier tokens can't cheat during training\n",
    "        if mask is not None:\n",
    "            mask = mask.to(device) # move your mask to the target device because a manually created tensor lives in the cpu by default\n",
    "            scaled_sims = scaled_sims.masked_fill(mask=mask, value=-1e9)  # -1e9 is an approximation of negative infinity\n",
    "\n",
    "        attention_percents = F.softmax(scaled_sims, dim=self.col_dim)\n",
    "\n",
    "        # attention_scores are the contextualised embeddings\n",
    "        attention_scores = torch.matmul(attention_percents, v)\n",
    "\n",
    "        return attention_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create MLP layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MlpBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Replicating the MLP block used in the transformer paper. It contains a linear layer and a \"add & norm\" (residual connection and layer norm) layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int = 2):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.linear_layer = nn.Linear(in_features=d_model,\n",
    "                                      out_features=d_model)\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape=d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Residual connection\n",
    "        x = self.layer_norm(x + self.linear_layer(x)) # operation fusion gives faster performance\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Encoder\n",
    "\n",
    "Workflow:\n",
    "1. Tokenize the source text\n",
    "2. Pass the tokens through the embedding layer\n",
    "3. Add positional encodings\n",
    "4. Pass the embeddings through the attention layer\n",
    "5. Pass the contextualised embeddings through the mlp block\n",
    "6. Return the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Replicating the encoder block in the transformer paper. It returns the contextualised embeddings of the source text tokens.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 num_tokens: int, \n",
    "                 d_model: int = 2, \n",
    "                 max_len: int = 6,\n",
    "                 device: torch.device = device):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Word Embeddings\n",
    "        self.we = nn.Embedding(num_embeddings=num_tokens, embedding_dim=d_model)\n",
    "\n",
    "        # Position Encodings\n",
    "        self.pe = PositionEncoding(d_model=d_model, max_len=max_len)\n",
    "\n",
    "        # Self-Attention\n",
    "        self.attention = AttentionBlock(d_model=d_model)\n",
    "\n",
    "        # MLP block\n",
    "        self.mlp_block = MlpBlock(d_model=d_model)\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "\n",
    "        # Create word embeddings\n",
    "        word_embeddings = self.we(token_ids)\n",
    "\n",
    "        # Add position encodings to the word embeddings\n",
    "        position_encoded = self.pe(word_embeddings)\n",
    "\n",
    "        # Create Self-Attention layers\n",
    "        self_attention_values = self.attention(position_encoded,\n",
    "                                               position_encoded,\n",
    "                                               position_encoded,\n",
    "                                               mask=None) # no mask is needed\n",
    "        \n",
    "        # Add residual connections and normalise\n",
    "        mlp_block_out = self.mlp_block(self_attention_values)\n",
    "\n",
    "        return mlp_block_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2405,  0.0951,  1.3264, -1.0392, -0.3596,  1.2178],\n",
       "        [-1.3465,  0.0830,  1.3067, -0.9236, -0.3445,  1.2249],\n",
       "        [-1.2227,  0.0731,  1.4018, -0.8164, -0.6422,  1.2064],\n",
       "        [-0.1274, -0.0895,  1.1554,  0.6726, -2.0077,  0.3966],\n",
       "        [-0.5583, -0.0343,  1.4319,  0.1238, -1.7379,  0.7749]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the encoder\n",
    "encoder = EncoderBlock(num_tokens=len(token_to_id_src),\n",
    "                       d_model=6,\n",
    "                       max_len=6)\n",
    "\n",
    "# Forward pass\n",
    "encoder_output = encoder(inputs[0])\n",
    "encoder_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Decoder\n",
    "\n",
    "Workflow:\n",
    "1. Tokenize the target text\n",
    "2. Pass the tokens through the embedding layer\n",
    "3. Add positional encodings\n",
    "4. Pass them through the masked attention layer\n",
    "5. Pass them through the mlp block\n",
    "6. Pass them through the encoder-decoder attention layer\n",
    "7. Pass them through the mlp block again\n",
    "8. Return the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Replicating the decoder block in the transformer paper. It returns the contextualised embeddings of the target text tokens.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 num_tokens: int, \n",
    "                 d_model: int = 2, \n",
    "                 max_len: int = 6,\n",
    "                 device: torch.device = device):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Word Embeddings\n",
    "        self.we = nn.Embedding(num_embeddings=num_tokens, embedding_dim=d_model)\n",
    "\n",
    "        # Position Encodings\n",
    "        self.pe = PositionEncoding(d_model=d_model, max_len=max_len)\n",
    "\n",
    "        # Attention block\n",
    "        self.attention = AttentionBlock(d_model=d_model)\n",
    "\n",
    "        # MLP block\n",
    "        self.mlp_block = MlpBlock(d_model=d_model)\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, \n",
    "                token_ids,\n",
    "                encoder_embeddings):\n",
    "\n",
    "        # Create word embeddings\n",
    "        word_embeddings = self.we(token_ids)\n",
    "\n",
    "        # Add position encodings to the word embeddings\n",
    "        decoder_embedding = self.pe(word_embeddings)\n",
    "\n",
    "        # Create a mask matrix for masking used in masked self attention\n",
    "        mask = torch.tril(torch.ones(token_ids.size(dim=0),token_ids.size(dim=0))) == 0 # the shape of mask is: [seq_len, seq_len]\n",
    "\n",
    "        # Create Masked-Attention layer\n",
    "        decoder_attention_values = self.attention(decoder_embedding,\n",
    "                                                          decoder_embedding,\n",
    "                                                          decoder_embedding,\n",
    "                                                          mask=mask,\n",
    "                                                          device=self.device)\n",
    "        \n",
    "        # Add residual connections and normalise\n",
    "        mlp_block_out1 = self.mlp_block(decoder_attention_values)\n",
    "\n",
    "        # Create Encoder-Decoder-Attention layer\n",
    "        encoder_decoder_attention_values = self.attention(mlp_block_out1, # decoder's queries\n",
    "                                                          encoder_embeddings, # encoder's keys\n",
    "                                                          encoder_embeddings, # encoder's values\n",
    "                                                          mask=None) # mask is not needed\n",
    "        \n",
    "        # Add residual connections and normalise\n",
    "        mlp_block_out2 = self.mlp_block(encoder_decoder_attention_values)\n",
    "\n",
    "        return mlp_block_out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1349, -0.4699,  0.0315, -1.7874,  1.4807,  0.6102],\n",
       "        [ 0.1382, -0.4721,  0.0276, -1.7883,  1.4746,  0.6200],\n",
       "        [ 0.1535, -0.4826,  0.0079, -1.7921,  1.4431,  0.6702],\n",
       "        [ 0.1941, -0.5069, -0.0429, -1.7965,  1.3586,  0.7937]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the decoder\n",
    "decoder = DecoderBlock(num_tokens=len(token_to_id_target),\n",
    "                       d_model=6,\n",
    "                       max_len=6,\n",
    "                       device=\"cpu\")\n",
    "decoder(token_ids=labels[0],\n",
    "        encoder_embeddings=encoder_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Transformer\n",
    "\n",
    "Now it's time to put all the puzzles together\n",
    "\n",
    "Workflow:\n",
    "1. Pass the tokenized sequence in source language through the encoder block\n",
    "2. Pass the tokenized sequence in target language through the decoder block\n",
    "3. Pass the result from the decoder through the last linear layer\n",
    "4. Run the resullt through a softmax layer\n",
    "5. Return the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(L.LightningModule):\n",
    "    \"\"\"Replicate the transformer architecture from the paper.\"\"\"\n",
    "    def __init__(self,\n",
    "                 num_tokens_src: int, # number of vocabs in the source language\n",
    "                 num_tokens_target: int, # number of vocabs in the target language\n",
    "                 d_model: int = 2,\n",
    "                 max_len: int = 6,\n",
    "                 device: torch.device = device):\n",
    "        super().__init__()\n",
    "\n",
    "        # Create an encoder block\n",
    "        self.encoder_block = EncoderBlock(\n",
    "            num_tokens=num_tokens_src,\n",
    "            d_model=d_model,\n",
    "            max_len=max_len,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        # Create a decoder block\n",
    "        self.decoder_block = DecoderBlock(\n",
    "            num_tokens=num_tokens_target,\n",
    "            d_model=d_model,\n",
    "            max_len=max_len,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        # Create the final linear layer\n",
    "        self.final_linear_layer = nn.Linear(in_features=d_model,\n",
    "                                            out_features=num_tokens_target) # output the probabilities for each vocab in the target tokenizer\n",
    "        \n",
    "        # Setup loss function\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, token_ids_src, token_ids_target): # the target sequence is also needed during training for teacher forcing\n",
    "        \n",
    "        # Pass the source tokens to the encoder block\n",
    "        contextualised_embeddings_encoder = self.encoder_block(token_ids_src)\n",
    "\n",
    "        # Pass the target sequence to the decoder block\n",
    "        contextualised_embeddings_decoder = self.decoder_block(token_ids=token_ids_target,\n",
    "                                                               encoder_embeddings=contextualised_embeddings_encoder)\n",
    "        \n",
    "        # Pass the result from decoder to the linear layer and then run it through a softmax layer\n",
    "        return self.final_linear_layer(contextualised_embeddings_decoder)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return Adam(self.parameters(), lr=0.1)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_tokens, labels = batch\n",
    "        # print(f\"input shape: {input_tokens.shape}, labels shape: {labels.shape}\")\n",
    "        outputs = self.forward(token_ids_src=input_tokens[0],\n",
    "                               token_ids_target=labels[0])\n",
    "        loss = self.loss(outputs, labels[0])\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2121,  0.0463, -0.5916, -0.0649, -0.1992, -0.1099],\n",
       "        [ 0.2146,  0.0508, -0.5983, -0.0682, -0.1910, -0.1035],\n",
       "        [ 0.2152,  0.0506, -0.5924, -0.0672, -0.1980, -0.1032],\n",
       "        [ 0.2148,  0.0473, -0.5745, -0.0625, -0.2190, -0.1069]],\n",
       "       device='mps:0', grad_fn=<LinearBackward0>)"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the transformer\n",
    "model = Transformer(num_tokens_src=len(token_to_id_src),\n",
    "                    num_tokens_target=len(token_to_id_target),\n",
    "                    d_model=6,\n",
    "                    max_len=6,\n",
    "                    device=device).to(device)\n",
    "model(token_ids_src=inputs[0].to(device),\n",
    "      token_ids_target=labels[0].to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model with `Lightning.Trainer.fit()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Create dataloader\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "dataset = TensorDataset(inputs, labels)\n",
    "dataloader = DataLoader(dataset)\n",
    "len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name               | Type             | Params | Mode \n",
      "----------------------------------------------------------------\n",
      "0 | encoder_block      | EncoderBlock     | 198    | train\n",
      "1 | decoder_block      | DecoderBlock     | 198    | train\n",
      "2 | final_linear_layer | Linear           | 42     | train\n",
      "3 | loss               | CrossEntropyLoss | 0      | train\n",
      "----------------------------------------------------------------\n",
      "438       Trainable params\n",
      "0         Non-trainable params\n",
      "438       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa340ebf97c04bf99fd1550f4622f667",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=60` reached.\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of trainer\n",
    "trainer = L.Trainer(max_epochs=60,\n",
    "                    accelerator=\"auto\")\n",
    "trainer.fit(model,\n",
    "            train_dataloaders=dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4755, -0.2123, -0.2173, -0.2164, -0.2187,  0.4778],\n",
       "        [ 0.4755, -0.2123, -0.2173, -0.2164, -0.2187,  0.4778],\n",
       "        [ 0.4755, -0.2123, -0.2173, -0.2164, -0.2187,  0.4778],\n",
       "        [ 0.4755, -0.2123, -0.2173, -0.2164, -0.2187,  0.4778]],\n",
       "       device='mps:0', grad_fn=<LinearBackward0>)"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the transformer\n",
    "model.to(device)\n",
    "model(token_ids_src=inputs[0].to(device),\n",
    "      token_ids_target=labels[0].to(device))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
