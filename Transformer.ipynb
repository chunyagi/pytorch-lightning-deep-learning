{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder–Decoder Transformers\n",
    "\n",
    "This notebook demonstrates how to build a very simple encoder–decoder Transformer from scratch, following the foundational ideas of the paper “Attention Is All You Need.” The encoder–decoder architecture underpins many modern machine translation systems, as well as diverse NLP tasks. By working through this notebook, you’ll see how the encoder processes the input to capture essential context, and how the decoder then generates the output—providing a solid grounding in how this classic Transformer design operates under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import lightning as L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create training dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two mappings (for encoder and decoder) from vocabs to numbers as nn.Embedding can only take integers\n",
    "token_to_id_eng = {\"<SOS>\": 0,\n",
    "                   \"let's\": 1,\n",
    "                   \"go\": 2,\n",
    "                   \"love\": 3,\n",
    "                   \"you\": 4,\n",
    "                   \"<EOS>\": 5}\n",
    "\n",
    "token_to_id_spa = {\"<SOS>\": 0,\n",
    "                   \"ir\" : 1,\n",
    "                   \"vamos\": 2,\n",
    "                   \"te\": 3,\n",
    "                   \"amo\": 4,\n",
    "                   \"<EOS>\": 5}\n",
    "                  \n",
    "# Create a mapping from numbers back to spanish vocabs in order to interpret the output from the transformer\n",
    "id_to_token_spa = dict(map(reversed, token_to_id_spa.items()))\n",
    "\n",
    "# This is actually not needed\n",
    "id_to_token_eng = dict(map(reversed, token_to_id_eng.items()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training pytorch dataset\n",
    "# As the input is going to be word embeddings, we only need the corresponding numbers from the mapping\n",
    "\n",
    "inputs = torch.tensor([[token_to_id_eng[\"let's\"],\n",
    "                        token_to_id_eng[\"go\"],\n",
    "                        token_to_id_eng[\"<EOS>\"],\n",
    "                        token_to_id_spa[\"<SOS>\"],\n",
    "                        token_to_id_spa[\"ir\"],\n",
    "                        token_to_id_spa[\"vamos\"]],\n",
    "                        \n",
    "                       [token_to_id_eng[\"love\"],\n",
    "                        token_to_id_eng[\"you\"],\n",
    "                        token_to_id_eng[\"<EOS>\"],\n",
    "                        token_to_id_spa[\"<SOS>\"],\n",
    "                        token_to_id_spa[\"te\"],\n",
    "                        token_to_id_spa[\"amo\"]]])\n",
    "\n",
    "labels = torch.tensor([[token_to_id_spa[\"ir\"],\n",
    "                        token_to_id_spa[\"vamos\"],\n",
    "                        token_to_id_spa[\"<EOS>\"]], \n",
    "                         \n",
    "                       [token_to_id_spa[\"te\"],\n",
    "                        token_to_id_spa[\"amo\"],\n",
    "                        token_to_id_spa[\"<EOS>\"]]])\n",
    "\n",
    "dataset = TensorDataset(inputs, labels)\n",
    "dataloader = DataLoader(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Position Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formula for the (standard, used in the paper **Attention is all you need**) position encoding is:  \n",
    "PE_(pos, 2i) = sin(pos / 10000^(2i / d_model))  \n",
    "PE_(pos, 2i+1) = cos(pos / 10000^(2i / d_model))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model=2, max_len=6):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # pe stands for position encoding\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "\n",
    "        # position is a column matrix (2D) of size [max_len, 1], e.g. [[0.], [1.], [2.]]\n",
    "        position = torch.arange(start=0, end=max_len, step=1).float().unsqueeze(1)\n",
    "\n",
    "        # Step is set to 2 because of \"2i\" in the formula, note that it is a 1D tensor, e.g. [0., 2.] as each position can have multiple embedding values\n",
    "        embedding_index = torch.arange(start=0, end=d_model, step=2).float()\n",
    "\n",
    "        # div_term is a row matrix (1D) with the same size as embedding_index\n",
    "        div_term = torch.tensor(10000.)**(embedding_index / d_model)\n",
    "\n",
    "        # Note: calculating the sin and cos values in this way only works when d_model is an even number, if d_model is odd, there will be a shape mismatch\n",
    "        pe[:, 0::2] = torch.sin(position / div_term)\n",
    "        pe[:, 1::2] = torch.cos(position / div_term)\n",
    "\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "\n",
    "    def forward(self, word_embeddings):\n",
    "\n",
    "        # Note: we might not need all the position encodings, as the number of tokens might not hit the maximum length (max_len)\n",
    "        return word_embeddings + self.pe[:word_embeddings.size(0), :]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention (including encoder/decoder (masked) self-attention and encoder-decoder attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model=2):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Create the weights associated with the query, key and value values\n",
    "        self.W_q = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        self.W_k = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        self.W_v = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "\n",
    "        self.row_dim = 0\n",
    "        self.col_dim = 1\n",
    "\n",
    "    def forward(self, encodings_for_q, encodings_for_k, encodings_for_v, mask=None):\n",
    "\n",
    "        # Create the Q, K and V matrices\n",
    "        q = self.W_q(encodings_for_q)\n",
    "        k = self.W_k(encodings_for_k)\n",
    "        v = self.W_v(encodings_for_v)\n",
    "\n",
    "        # Calculate the similarity score between the query values and key values\n",
    "        sims = torch.matmul(q, k.transpose(dim0=self.row_dim, dim1=self.col_dim))\n",
    "\n",
    "        # Scale the similarity score with the square root of d_model\n",
    "        scaled_sims = sims / torch.tensor((k.size(self.col_dim))**0.5)\n",
    "\n",
    "        device = scaled_sims.device\n",
    "        # Move your mask to mps:0, or mask would live in cpu by default\n",
    "        \n",
    "        # Mask the scaled similarity scores of the later tokens so that the earlier tokens can't cheat. Note: -1e9 is an approximation of negative infinity\n",
    "        if mask is not None:\n",
    "            mask = mask.to(device)\n",
    "            scaled_sims = scaled_sims.masked_fill(mask=mask, value=-1e9)\n",
    "\n",
    "        # Applying the softmax function to the scaled similarites determines the percentages of influence each token (in columns) should have on the others (in rows)\n",
    "        attention_percents = F.softmax(scaled_sims, dim=self.col_dim)\n",
    "\n",
    "        # attention_scores are basically the contextualised embeddings\n",
    "        attention_scores = torch.matmul(attention_percents, v)\n",
    "\n",
    "        return attention_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, num_tokens, d_model, max_len):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Word Embeddings\n",
    "        self.we = nn.Embedding(num_embeddings=num_tokens, embedding_dim=d_model)\n",
    "\n",
    "        # Position Encodings\n",
    "        self.pe = PositionEncoding(d_model=d_model, max_len=max_len)\n",
    "\n",
    "        # Encoder Self-Attention\n",
    "        self.attention = Attention(d_model=d_model)\n",
    "\n",
    "        # Fully Connected layer, it's commented out here for simplicity\n",
    "        # self.fc = nn.Linear(in_features=d_model, out_features=d_model)\n",
    "\n",
    "    # The size of token_ids just needs to be a 1D tensor (without batching)\n",
    "    def forward(self, token_ids):\n",
    "\n",
    "        # Create word embeddings\n",
    "        word_embeddings = self.we(token_ids)\n",
    "\n",
    "        # Add position encodings to the word embeddings\n",
    "        position_encoded = self.pe(word_embeddings)\n",
    "\n",
    "        # Self-Attention\n",
    "        self_attention_values = self.attention(position_encoded,\n",
    "                                               position_encoded,\n",
    "                                               position_encoded,\n",
    "                                               mask=None)\n",
    "\n",
    "        # Add residual connections\n",
    "        residual_connection_values = position_encoded + self_attention_values\n",
    "\n",
    "        # Run the residual connections through a fully connected layer\n",
    "        # fc_layer_out = self.fc(residual_connection_values)\n",
    "\n",
    "        # Return the residual connections\n",
    "        return residual_connection_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    # Actually, we can further categorise these parameters (encoder- and decoder-)\n",
    "    def __init__(self, num_tokens, d_model, max_len):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Word Embeddings\n",
    "        self.we = nn.Embedding(num_embeddings=num_tokens, embedding_dim=d_model)\n",
    "\n",
    "        # Position Encodings\n",
    "        self.pe = PositionEncoding(d_model=d_model, max_len=max_len)\n",
    "\n",
    "        # Decoder Self-Attention\n",
    "        self.self_attention = Attention(d_model=d_model)\n",
    "\n",
    "        # Encoder-Decoder Attention\n",
    "        # Note: it needs to be separated from the decoder self-attention as the weights are different\n",
    "        self.encoder_decoder_attention = Attention(d_model=d_model)\n",
    "\n",
    "    # The size of token_ids just needs to be a 1D tensor (without batching)\n",
    "    def forward(self, token_ids, embeddings_for_k, embeddings_for_v):\n",
    "        \n",
    "        # Create word embeddings\n",
    "        word_embeddings = self.we(token_ids)\n",
    "\n",
    "        # Add position encodings to the word embeddings\n",
    "        position_encoded = self.pe(word_embeddings)\n",
    "\n",
    "        # Create a mask matrix for masking used in decoder self attention\n",
    "        # The size of mask should be [decoder_seq_len, decoder_seq_len]\n",
    "        mask = torch.tril(torch.ones(token_ids.size(dim=0),token_ids.size(dim=0))) == 0\n",
    "\n",
    "        # Decoder Self-Attention\n",
    "        self_attention_values = self.self_attention(position_encoded,\n",
    "                                                    position_encoded,\n",
    "                                                    position_encoded,\n",
    "                                                    mask=mask)\n",
    "\n",
    "        # Add residual connections\n",
    "        residual_connection_values = position_encoded + self_attention_values\n",
    "\n",
    "        # Encdoer-Decoder Attention\n",
    "        encoder_decoder_attention_values = self.encoder_decoder_attention(position_encoded,\n",
    "                                                                          embeddings_for_k,\n",
    "                                                                          embeddings_for_v,\n",
    "                                                                          mask=None)\n",
    "        # Add residual connections\n",
    "        residual_connection_values = encoder_decoder_attention_values + residual_connection_values\n",
    "\n",
    "        return residual_connection_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(L.LightningModule):\n",
    "\n",
    "    # Note: these parameters can actually be further categorised as encoder's and decoder's, but we keep it simple here\n",
    "    def __init__(self, num_tokens, d_model, max_len):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = Encoder(num_tokens=num_tokens, d_model=d_model, max_len=max_len)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = Decoder(num_tokens=num_tokens, d_model=d_model, max_len=max_len)\n",
    "\n",
    "        # Fully Connected layer\n",
    "        self.fc = nn.Linear(in_features=d_model, out_features=num_tokens)\n",
    "\n",
    "        # Calculate the loss with Cross Entropy; softmax is already included\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "        # For the purpose of token generation\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "\n",
    "        # Note: token_ids here include both english and spanish token ids\n",
    "        # So we need to find where the english sentence ends first, note that [0] is needed or it will return a tuple instead\n",
    "        end_idx = torch.where(token_ids == token_to_id_spa[\"<SOS>\"])[0].item()\n",
    "        \n",
    "        # Get the contextualised embeddings from the encoder\n",
    "        contextualised_embeddings = self.encoder(token_ids[:end_idx])\n",
    "\n",
    "        # Get the residual connection values from the decoder for further processing\n",
    "        residual_connection_values = self.decoder(token_ids[end_idx:],\n",
    "                                                  contextualised_embeddings,\n",
    "                                                  contextualised_embeddings)\n",
    "        \n",
    "        # Run the residual connections through a fully connected layer\n",
    "        fc_layer_out = self.fc(residual_connection_values)\n",
    "\n",
    "        # Return the fully connected layer\n",
    "        return fc_layer_out\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return Adam(self.parameters(), lr=0.1)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \n",
    "        # input_tokens is a 2D tensor of size [batch_size, seq_len]\n",
    "        input_tokens, labels = batch\n",
    "        # print(batch, '\\n')\n",
    "        # print(input_tokens[0])\n",
    "        # outputs is fc_layer_out, so they share the same size\n",
    "        outputs = self.forward(input_tokens[0])\n",
    "        # Cross Entropy loss will automatically apply softmax to the outputs\n",
    "        loss = self.loss(outputs, labels[0])\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    # Inference\n",
    "    def generate(self, src_token_ids):\n",
    "\n",
    "        # Encode the source language sentence\n",
    "        contextualised_embeddings = self.encoder(src_token_ids)\n",
    "\n",
    "        # Remind the decoder to start decoding by feeding it a <SOS> token\n",
    "        # Note: target_token_ids needs to be 1D\n",
    "        target_token_ids = torch.tensor([token_to_id_spa[\"<SOS>\"]])\n",
    "\n",
    "        # Start decoding\n",
    "        residual_connection_values = self.decoder(target_token_ids,\n",
    "                                                  contextualised_embeddings,\n",
    "                                                  contextualised_embeddings)\n",
    "        \n",
    "        # Run the residual connections through a fully connected layer\n",
    "        fc_layer_out = self.fc(residual_connection_values)\n",
    "\n",
    "        # Run the output through a argmax layer (softmax is not needed anymore as no derivatives are required here) and then apply argmax to it to get the prediction\n",
    "        # Note: we need to make prediction_id a 1D tensor for the concatenation later\n",
    "        prediction_id = torch.tensor([torch.argmax(fc_layer_out)])\n",
    "        # prediction_ids = prediction_id\n",
    "\n",
    "        # Now we can start predicting the next token recursively\n",
    "        for i in range(self.max_len-1):\n",
    "\n",
    "            # Include the newly generated token into the input first so the decoder has full context\n",
    "            target_token_ids = torch.cat((target_token_ids, prediction_id))\n",
    "\n",
    "            # Check if the newly prediction_id is actually the one for <EOS>, if so, break the loop\n",
    "            if (prediction_id == token_to_id_spa[\"<EOS>\"]):\n",
    "                break\n",
    "            \n",
    "            # Start decoding\n",
    "            residual_connection_values = self.decoder(target_token_ids,\n",
    "                                                      contextualised_embeddings,\n",
    "                                                      contextualised_embeddings)\n",
    "            \n",
    "            # Run the residual connections through a fully connected layer\n",
    "            # Note we don't to predict the first token again from <SOS>, so we just to pass the last attention output values (token) to the fc layer\n",
    "            fc_layer_out = self.fc(residual_connection_values[-1:])\n",
    "            prediction_id = torch.tensor([torch.argmax(fc_layer_out)])\n",
    "\n",
    "        \n",
    "        # Print the output\n",
    "        print(\"Predicted Tokens:\\n\")\n",
    "        for id in target_token_ids:\n",
    "            print(\"\\t\", id_to_token_spa[id.item()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training (teacher forcing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(num_tokens=len(token_to_id_eng), d_model=2, max_len=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name    | Type             | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0 | encoder | Encoder          | 24     | train\n",
      "1 | decoder | Decoder          | 36     | train\n",
      "2 | fc      | Linear           | 18     | train\n",
      "3 | loss    | CrossEntropyLoss | 0      | train\n",
      "-----------------------------------------------------\n",
      "78        Trainable params\n",
      "0         Non-trainable params\n",
      "78        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "/Users/edison/Git/pytorch-playground/myenv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/Users/edison/Git/pytorch-playground/myenv/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "398710a892c04750a97dc3c283285803",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=30` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(max_epochs=30)\n",
    "trainer.fit(model, train_dataloaders=dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference (autoregressive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Tokens:\n",
      "\n",
      "\t <SOS>\n",
      "\t ir\n",
      "\t vamos\n",
      "\t <EOS>\n"
     ]
    }
   ],
   "source": [
    "# Test case 1\n",
    "test_input = torch.tensor([token_to_id_eng[\"let's\"],\n",
    "                           token_to_id_eng[\"go\"]])\n",
    "\n",
    "model.generate(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Tokens:\n",
      "\n",
      "\t <SOS>\n",
      "\t te\n",
      "\t amo\n",
      "\t <EOS>\n"
     ]
    }
   ],
   "source": [
    "# Test case 2\n",
    "test_input = torch.tensor([token_to_id_eng[\"love\"],\n",
    "                           token_to_id_eng[\"you\"]])\n",
    "\n",
    "model.generate(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Tokens:\n",
      "\n",
      "\t <SOS>\n",
      "\t amo\n",
      "\t te\n",
      "\t vamos\n",
      "\t <EOS>\n"
     ]
    }
   ],
   "source": [
    "# Test case 3\n",
    "test_input = torch.tensor([token_to_id_eng[\"go\"],\n",
    "                           token_to_id_eng[\"love\"],\n",
    "                           token_to_id_eng[\"you\"]])\n",
    "\n",
    "model.generate(test_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
