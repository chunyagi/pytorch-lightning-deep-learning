{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder-Only Transformers\n",
    "\n",
    "This notebook demonstrates how to build a very simple decoder-only Transformer from scratch. Decoder-only architectures, such as those powering large language models (LLMs) like ChatGPT, focus exclusively on the generative component of the Transformer. By working through this notebook, you'll see how these models process context and generate outputâ€”providing a foundation for understanding how modern LLMs operate under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import lightning as L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from vocabs to numbers as nn.Embedding can only take integers\n",
    "token_to_id = {\"what\": 0,\n",
    "               \"is\": 1,\n",
    "                \"statquest\": 2,\n",
    "                \"awesome\": 3,\n",
    "                \"<EOS>\": 4\n",
    "              }\n",
    "\n",
    "# Create a mapping from numbers back to vocabs to interpret the output from the transformer\n",
    "id_to_token = dict(map(reversed, token_to_id.items()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training dataset\n",
    "# As the input is going to be word embeddings, we only need the corresponding numbers from the mapping\n",
    "# The tokens used as inputs during training comes from 1. processing the prompt and 2. generating the output\n",
    "inputs = torch.tensor([[token_to_id[\"what\"],\n",
    "                        token_to_id[\"is\"],\n",
    "                        token_to_id[\"statquest\"],\n",
    "                        token_to_id[\"<EOS>\"],\n",
    "                        token_to_id[\"awesome\"]\n",
    "                        ], \n",
    "                        \n",
    "                        [token_to_id[\"statquest\"],\n",
    "                         token_to_id[\"is\"],\n",
    "                         token_to_id[\"what\"],\n",
    "                         token_to_id[\"<EOS>\"],\n",
    "                         token_to_id[\"awesome\"]]])\n",
    "\n",
    "labels = torch.tensor([[token_to_id[\"is\"],\n",
    "                        token_to_id[\"statquest\"],\n",
    "                        token_to_id[\"<EOS>\"],\n",
    "                        token_to_id[\"awesome\"],\n",
    "                        token_to_id[\"<EOS>\"]], \n",
    "                         \n",
    "                       [token_to_id[\"is\"],\n",
    "                        token_to_id[\"what\"],\n",
    "                        token_to_id[\"<EOS>\"],\n",
    "                        token_to_id[\"awesome\"],\n",
    "                        token_to_id[\"<EOS>\"]]])\n",
    "\n",
    "dataset = TensorDataset(inputs, labels)\n",
    "dataloader = DataLoader(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Position Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formula for the (standard, used in the paper **Attention is all you need**) position encoding is:  \n",
    "PE_(pos, 2i) = sin(pos / 10000^(2i / d_model))  \n",
    "PE_(pos, 2i+1) = cos(pos / 10000^(2i / d_model))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model=2, max_len=6):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # pe stands for position encoding\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "\n",
    "        # position is a column matrix (2D) of size [max_len, 1], e.g. [[0.], [1.], [2.]]\n",
    "        position = torch.arange(start=0, end=max_len, step=1).float().unsqueeze(1)\n",
    "\n",
    "        # Step is set to 2 because of \"2i\" in the formula, note that it is a 1D tensor, e.g. [0., 2.] as each position can have multiple embedding values\n",
    "        embedding_index = torch.arange(start=0, end=d_model, step=2).float()\n",
    "\n",
    "        # div_term is a row matrix (1D) with the same size as embedding_index\n",
    "        div_term = torch.tensor(10000.)**(embedding_index / d_model)\n",
    "\n",
    "        # Note: calculating the sin and cos values in this way only works when d_model is an even number, if d_model is odd, there will be a shape mismatch\n",
    "        pe[:, 0::2] = torch.sin(position / div_term)\n",
    "        pe[:, 1::2] = torch.cos(position / div_term)\n",
    "\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "\n",
    "    def forward(self, word_embeddings):\n",
    "\n",
    "        # Note: we might not need all the position encodings, as the number of tokens might not hit the maximum length (max_len)\n",
    "        return word_embeddings + self.pe[:word_embeddings.size(0), :]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masked Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model=2):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Create the weights associated with the query, key and value values\n",
    "        self.W_q = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        self.W_k = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        self.W_v = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "\n",
    "        self.row_dim = 0\n",
    "        self.col_dim = 1\n",
    "\n",
    "    def forward(self, encodings_for_q, encodings_for_k, encodings_for_v, mask=None):\n",
    "\n",
    "        # Create the Q, K and V matrices\n",
    "        q = self.W_q(encodings_for_q)\n",
    "        k = self.W_k(encodings_for_k)\n",
    "        v = self.W_v(encodings_for_v)\n",
    "\n",
    "        # Calculate the similarity score between the query values and key values\n",
    "        sims = torch.matmul(q, k.transpose(dim0=self.row_dim, dim1=self.col_dim))\n",
    "\n",
    "        # Scale the similarity score with the square root of d_model\n",
    "        scaled_sims = sims / torch.tensor((k.size(self.col_dim))**0.5)\n",
    "\n",
    "        device = scaled_sims.device\n",
    "        # Move your mask to mps:0, or mask would live in cpu by default\n",
    "        mask = mask.to(device)\n",
    "\n",
    "        # Mask the scaled similarity scores of the later tokens so that the earlier tokens can't cheat. Note: -1e9 is an approximation of negative infinity\n",
    "        if mask is not None:\n",
    "            scaled_sims = scaled_sims.masked_fill(mask=mask, value=-1e9)\n",
    "\n",
    "        # Applying the softmax function to the scaled similarites determines the percentages of influence each token (in columns) should have on the others (in rows)\n",
    "        attention_percents = F.softmax(scaled_sims, dim=self.col_dim)\n",
    "\n",
    "        # attention_scores are basically the contextualised embeddings\n",
    "        attention_scores = torch.matmul(attention_percents, v)\n",
    "\n",
    "        return attention_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder-only Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderOnlyTransformer(L.LightningModule):\n",
    "\n",
    "    def __init__(self, num_tokens, d_model, max_len):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Word Embeddings\n",
    "        self.we = nn.Embedding(num_embeddings=num_tokens, embedding_dim=d_model)\n",
    "\n",
    "        # Position Encodings\n",
    "        self.pe = PositionEncoding(d_model=d_model, max_len=max_len)\n",
    "\n",
    "        # Masked Self-Attention\n",
    "        self.attention = Attention(d_model=d_model)\n",
    "\n",
    "        # Fully Connected layer\n",
    "        self.fc = nn.Linear(in_features=d_model, out_features=num_tokens)\n",
    "\n",
    "        # Calculate the loss with Cross Entropy; softmax is already included\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    # The size of token_ids just needs to be a 1D tensor (without batching), unlike nn.LSTM, which requires the size of the input tensor to be [seq_len, batch_size, input_size]\n",
    "    def forward(self, token_ids):\n",
    "\n",
    "        # Create word embeddings\n",
    "        word_embeddings = self.we(token_ids)\n",
    "\n",
    "        # Add position encodings to the word embeddings\n",
    "        position_encoded = self.pe(word_embeddings)\n",
    "\n",
    "        # Create a mask matrix for masking used in masked self attention\n",
    "        # The size of mask should be [seq_len, seq_len]\n",
    "        mask = torch.tril(torch.ones(token_ids.size(dim=0),token_ids.size(dim=0))) == 0\n",
    "\n",
    "        # Masked Self-Attention\n",
    "        self_attention_values = self.attention(position_encoded,\n",
    "                                               position_encoded,\n",
    "                                               position_encoded,\n",
    "                                               mask=mask)\n",
    "\n",
    "        # Add residual connections\n",
    "        residual_connection_values = position_encoded + self_attention_values\n",
    "\n",
    "        # Run the residual connections through a fully connected layer\n",
    "        # fc_layer_out has the same size as residual_connected_values, which is a 2D tensor of size [seq_len, num_tokens]\n",
    "        fc_layer_out = self.fc(residual_connection_values)\n",
    "\n",
    "        # Return the fully connected layer\n",
    "        return fc_layer_out\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return Adam(self.parameters(), lr=0.1)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \n",
    "        # input_tokens is a 2D tensor of size [batch_size, seq_len]\n",
    "        input_tokens, labels = batch\n",
    "        # print(batch, '\\n')\n",
    "        # print(input_tokens[0])\n",
    "        # outputs is fc_layer_out, so they share the same size\n",
    "        outputs = self.forward(input_tokens[0])\n",
    "        # Cross Entropy loss will automatically apply softmax to the outputs\n",
    "        loss = self.loss(outputs, labels[0])\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we train the model, let's see what the model outputs for fun\n",
    "model = DecoderOnlyTransformer(num_tokens=len(token_to_id), d_model=2, max_len=6)\n",
    "\n",
    "# Create a prompt\n",
    "model_input = torch.tensor([token_to_id[\"what\"],\n",
    "                            token_to_id[\"is\"],\n",
    "                            token_to_id[\"statquest\"],\n",
    "                            token_to_id[\"<EOS>\"]])\n",
    "\n",
    "input_length = model_input.size(dim=0)\n",
    "\n",
    "# predictions is the raw score output by the fulled connected layer\n",
    "predictions = model(model_input)\n",
    "# predictions is a 2D tensor of size [seq_len (or input_length), num_tokens (or vocab_size)] orginally, however, we are only interested in the prediction of the last token in the prompt\n",
    "# So now predicted_id is a 1D tensor of size [1]\n",
    "predicted_id = torch.tensor([torch.argmax(predictions[-1,:])])\n",
    "# Used to store the coming predicted_id's during inference\n",
    "predicted_ids = predicted_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Tokens:\n",
      "\n",
      "\t awesome\n",
      "\t awesome\n",
      "\t awesome\n"
     ]
    }
   ],
   "source": [
    "# Generate output\n",
    "max_len = 6\n",
    "\n",
    "for i in range(input_length, max_len):\n",
    "    # Check if the predicted_id is actually the one for <EOS>, if yes, break the loop\n",
    "    # This condition is actually a bit sloppy, it is comparing a 1D tensor against an integer, but it works, and it returns a 1D tensor like tensor([True])\n",
    "    if (predicted_id == token_to_id[\"<EOS>\"]):\n",
    "        break\n",
    "\n",
    "    # if not, continue generating the next token\n",
    "    # But first, include the newly generated token into the input first so the model has full context\n",
    "    model_input = torch.cat((model_input, predicted_id))\n",
    "    predictions = model(model_input)\n",
    "    predicted_id = torch.tensor([torch.argmax(predictions[-1,:])])\n",
    "    predicted_ids = torch.cat((predicted_id, predicted_ids))\n",
    "\n",
    "# Print the output\n",
    "print(\"Predicted Tokens:\\n\")\n",
    "for id in predicted_ids:\n",
    "    # Don't forget id is a 0D tensor actually, i.e. tensor(0) because it is an output from torch.argmax\n",
    "    print(\"\\t\", id_to_token[id.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | we        | Embedding        | 10     | train\n",
      "1 | pe        | PositionEncoding | 0      | train\n",
      "2 | attention | Attention        | 12     | train\n",
      "3 | fc        | Linear           | 15     | train\n",
      "4 | loss      | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "37        Trainable params\n",
      "0         Non-trainable params\n",
      "37        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "/Users/edison/Git/pytorch-playground/myenv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/Users/edison/Git/pytorch-playground/myenv/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "448fc6f54435459aaff81f8cf87df446",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=30` reached.\n"
     ]
    }
   ],
   "source": [
    "# That means we need to train the model...\n",
    "trainer = L.Trainer(max_epochs=30)\n",
    "trainer.fit(model, train_dataloaders=dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the same code after training\n",
    "# Test the model with the first prompt\n",
    "model_input = torch.tensor([token_to_id[\"what\"],\n",
    "                            token_to_id[\"is\"],\n",
    "                            token_to_id[\"statquest\"],\n",
    "                            token_to_id[\"<EOS>\"]])\n",
    "\n",
    "input_length = model_input.size(dim=0)\n",
    "\n",
    "# predictions is the raw score output by the fulled connected layer\n",
    "predictions = model(model_input)\n",
    "# predictions is a 2D tensor of size [seq_len (or input_length), num_tokens (or vocab_size)] orginally, however, we are only interested in the prediction of the last token in the prompt\n",
    "# So now predicted_id is a 1D tensor of size [1]\n",
    "predicted_id = torch.tensor([torch.argmax(predictions[-1,:])])\n",
    "# Used to store the coming predicted_id's during inference\n",
    "predicted_ids = predicted_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Tokens:\n",
      "\n",
      "\t awesome\n",
      "\t <EOS>\n"
     ]
    }
   ],
   "source": [
    "# Generate output\n",
    "for i in range(input_length, max_len):\n",
    "    # Check if the predicted_id is actually the one for <EOS>, if yes, break the loop\n",
    "    # This condition is actually a bit sloppy, it is comparing a 1D tensor against an integer, but it works, and it returns a 1D tensor like tensor([True])\n",
    "    if (predicted_id == token_to_id[\"<EOS>\"]):\n",
    "        break\n",
    "\n",
    "    # if not, continue generating the next token\n",
    "    # But first, include the newly generated token into the input first so the model has full context\n",
    "    model_input = torch.cat((model_input, predicted_id))\n",
    "    predictions = model(model_input)\n",
    "    predicted_id = torch.tensor([torch.argmax(predictions[-1,:])])\n",
    "    predicted_ids = torch.cat((predicted_ids, predicted_id))\n",
    "\n",
    "# Print the output\n",
    "print(\"Predicted Tokens:\\n\")\n",
    "for id in predicted_ids:\n",
    "    # Don't forget id is a 0D tensor actually, i.e. tensor(0) because it is an output from torch.argmax\n",
    "    print(\"\\t\", id_to_token[id.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model with the second prompt\n",
    "model_input = torch.tensor([token_to_id[\"statquest\"],\n",
    "                            token_to_id[\"is\"],\n",
    "                            token_to_id[\"what\"],\n",
    "                            token_to_id[\"<EOS>\"]])\n",
    "\n",
    "input_length = model_input.size(dim=0)\n",
    "\n",
    "# predictions is the raw score output by the fulled connected layer\n",
    "predictions = model(model_input)\n",
    "# predictions is a 2D tensor of size [seq_len (or input_length), num_tokens (or vocab_size)] orginally, however, we are only interested in the prediction of the last token in the prompt\n",
    "# So now predicted_id is a 1D tensor of size [1] for the concatenation of predicted_id and predicted_ids (1D) later\n",
    "predicted_id = torch.tensor([torch.argmax(predictions[-1,:])])\n",
    "# Used to store the coming predicted_id's during inference\n",
    "predicted_ids = predicted_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Tokens:\n",
      "\n",
      "\t awesome\n",
      "\t <EOS>\n"
     ]
    }
   ],
   "source": [
    "# Generate output\n",
    "for i in range(input_length, max_len):\n",
    "    # Check if the predicted_id is actually the one for <EOS>, if yes, break the loop\n",
    "    # This condition is actually a bit sloppy, it is comparing a 1D tensor against an integer, but it works, and it returns a 1D tensor like tensor([True])\n",
    "    if (predicted_id == token_to_id[\"<EOS>\"]):\n",
    "        break\n",
    "\n",
    "    # if not, continue generating the next token\n",
    "    # But first, include the newly generated token into the input first so the model has full context\n",
    "    model_input = torch.cat((model_input, predicted_id))\n",
    "    predictions = model(model_input)\n",
    "    predicted_id = torch.tensor([torch.argmax(predictions[-1,:])])\n",
    "    predicted_ids = torch.cat((predicted_ids, predicted_id))\n",
    "\n",
    "# Print the output\n",
    "print(\"Predicted Tokens:\\n\")\n",
    "for id in predicted_ids:\n",
    "    # Don't forget id is a 0D tensor actually, i.e. tensor(0) because it is an output from torch.argmax\n",
    "    print(\"\\t\", id_to_token[id.item()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
