{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replicate the Decoder-Only Transformers architecture with PyTorch + Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import lightning as L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup device-agnostic code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'what', 1: 'is', 2: 'robot', 3: 'awesome', 4: '<EOS>'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a mapping from vocabs to numbers to mimic the tokenizer\n",
    "token_to_id = {\n",
    "    \"what\": 0,\n",
    "    \"is\": 1,\n",
    "    \"robot\": 2,\n",
    "    \"awesome\": 3,\n",
    "    \"<EOS>\": 4,\n",
    "}\n",
    "\n",
    "# Create a mapping from numbers back to vocabs for interpreting the output from the transformer later\n",
    "id_to_token = dict(map(reversed, token_to_id.items()))\n",
    "id_to_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a training dataset\n",
    "\n",
    "**Note:** the transformer processes all tokens in parallel (only) during training (or when using teacher forcing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([\n",
    "    [\n",
    "        token_to_id[\"what\"],\n",
    "        token_to_id[\"is\"],\n",
    "        token_to_id[\"robot\"],\n",
    "        token_to_id[\"<EOS>\"],\n",
    "        token_to_id[\"awesome\"]\n",
    "    ],\n",
    "    \n",
    "    [\n",
    "        token_to_id[\"robot\"],\n",
    "        token_to_id[\"is\"],\n",
    "        token_to_id[\"what\"],\n",
    "        token_to_id[\"<EOS>\"],\n",
    "        token_to_id[\"awesome\"],\n",
    "    ]\n",
    "])\n",
    "\n",
    "labels = torch.tensor([\n",
    "    [\n",
    "        token_to_id[\"is\"],\n",
    "        token_to_id[\"robot\"],\n",
    "        token_to_id[\"<EOS>\"],\n",
    "        token_to_id[\"awesome\"],\n",
    "        token_to_id[\"<EOS>\"]\n",
    "    ],\n",
    "\n",
    "    [\n",
    "        token_to_id[\"is\"],\n",
    "        token_to_id[\"what\"],\n",
    "        token_to_id[\"<EOS>\"],\n",
    "        token_to_id[\"awesome\"],\n",
    "        token_to_id[\"<EOS>\"],\n",
    "    ]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create position encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the formula used in the paper *Attention is all you need* position encoding is:\n",
    "* PE_(pos, 2i) = sin(pos / 10000^(2i / d_model))\n",
    "* PE_(pos, 2i+1) = cos(pos / 10000^(2i / d_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model=2, max_len=6):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "\n",
    "        position = torch.arange(start=0, end=max_len, step=1).float().unsqueeze(1)\n",
    "\n",
    "        embedding_index = torch.arange(start=0, end=d_model, step=2).float()\n",
    "\n",
    "        div_term = torch.tensor(10000)**(embedding_index / d_model)\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position / div_term)\n",
    "        pe[:, 1::2] = torch.cos(position / div_term)\n",
    "\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "    \n",
    "    def forward(self, word_embeddings):\n",
    "\n",
    "        return word_embeddings + self.pe[:word_embeddings.size(0), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create masked Self-Attention layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model=2):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.W_q = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        self.W_k = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        self.W_v = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "\n",
    "        self.row_dim = 0\n",
    "        self.col_dim = 1\n",
    "\n",
    "    def forward(self, encodings_for_q, encodings_for_k, encodings_for_v, mask=None, device=device):\n",
    "\n",
    "        # Create the Q, K and V matrices\n",
    "        q = self.W_q(encodings_for_q)\n",
    "        k = self.W_k(encodings_for_k)\n",
    "        v = self.W_v(encodings_for_v)\n",
    "\n",
    "        # Calculate the similarity score between the queries and values\n",
    "        sims = torch.matmul(q, k.transpose(dim0=self.row_dim, dim1=self.col_dim))\n",
    "\n",
    "        # Scale the similarity score with the square root of d_model\n",
    "        scaled_sims = sims / torch.tensor((k.size(self.col_dim))**0.5)\n",
    "\n",
    "        # Mask the scaled similarity scores of the later tokens so that the earlier tokens can't cheat during training\n",
    "        if mask is not None:\n",
    "            # Move your mask to the target device because a manually created tensor lives in the cpu by default\n",
    "            mask = mask.to(device)\n",
    "            scaled_sims = scaled_sims.masked_fill(mask=mask, value=-1e9)  # -1e9 is an approximation of negative infinity\n",
    "\n",
    "        attention_percents = F.softmax(scaled_sims, dim=self.col_dim)\n",
    "\n",
    "        # attention_scores are the contextualised embeddings\n",
    "        attention_scores = torch.matmul(attention_percents, v)\n",
    "\n",
    "        return attention_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Decoder-only Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderOnlyTransformer(L.LightningModule):\n",
    "\n",
    "    def __init__(self, num_tokens, d_model, max_len):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Word Embeddings\n",
    "        self.we = nn.Embedding(num_embeddings=num_tokens, embedding_dim=d_model)\n",
    "\n",
    "        # Position Encodings\n",
    "        self.pe = PositionEncoding(d_model=d_model, max_len=max_len)\n",
    "\n",
    "        # Masked Self-Attention\n",
    "        self.attention = Attention(d_model=d_model)\n",
    "\n",
    "        # Fully Connected layer\n",
    "        self.fc = nn.Linear(in_features=d_model, out_features=num_tokens)\n",
    "\n",
    "        self.loss = nn.CrossEntropyLoss() # softmax is included\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        \n",
    "        # print(f\"The model is training on: {next(self.parameters()).device}\")\n",
    "\n",
    "        # Create word embeddings\n",
    "        word_embeddings = self.we(token_ids)\n",
    "\n",
    "        # Add position encodings to the word embeddings\n",
    "        position_encoded = self.pe(word_embeddings)\n",
    "\n",
    "        # Create a mask matrix for masking used in masked self attention\n",
    "        mask = torch.tril(torch.ones(token_ids.size(dim=0),token_ids.size(dim=0))) == 0 # the shape of mask is: [seq_len, seq_len]\n",
    "\n",
    "        # Masked Self-Attention\n",
    "        self_attention_values = self.attention(position_encoded,\n",
    "                                               position_encoded,\n",
    "                                               position_encoded,\n",
    "                                               mask=mask)\n",
    "\n",
    "        # Add residual connections\n",
    "        residual_connection_values = position_encoded + self_attention_values\n",
    "\n",
    "        # Run the residual connections through a fully connected layer\n",
    "        fc_layer_out = self.fc(residual_connection_values)\n",
    "\n",
    "        return fc_layer_out\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return Adam(self.parameters(), lr=0.1)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # input_tokens comes in a batch: [batch_size, seq_len], with batch_size=1\n",
    "        input_tokens, labels = batch\n",
    "        outputs = self.forward(input_tokens[0])\n",
    "        loss = self.loss(outputs, labels[0])\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model predictions (logits) before training:\n",
      " tensor([[-0.3656, -0.2974,  0.1898,  0.5190, -0.1002],\n",
      "        [-0.8329, -0.4573,  0.0647,  0.4218, -0.2724],\n",
      "        [ 0.2757,  0.4283, -0.2465,  0.1536, -0.4414],\n",
      "        [-0.5160, -0.1763, -0.0577,  0.3177, -0.3525],\n",
      "        [ 0.0559, -0.4981,  0.7168,  0.9466,  0.4487]], device='mps:0',\n",
      "       grad_fn=<LinearBackward0>)\n",
      "\n",
      "Model predictions (probabilities) before training:\n",
      " tensor([[0.1326, 0.1420, 0.2311, 0.3213, 0.1730],\n",
      "        [0.0983, 0.1432, 0.2413, 0.3449, 0.1723],\n",
      "        [0.2421, 0.2820, 0.1436, 0.2142, 0.1182],\n",
      "        [0.1340, 0.1881, 0.2118, 0.3083, 0.1577],\n",
      "        [0.1346, 0.0774, 0.2607, 0.3280, 0.1994]], device='mps:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "\n",
      "Model predictions (vocab) before training:\n",
      " ['awesome', 'awesome', 'is', 'awesome', 'awesome']\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "model = DecoderOnlyTransformer(num_tokens=len(token_to_id), d_model=2, max_len=6).to(device) # move the model to mps\n",
    "\n",
    "# Create a prompt\n",
    "model_input = inputs[0].to(device)\n",
    "\n",
    "input_length = model_input.size(dim=0)\n",
    "\n",
    "predictions = model(model_input) # the logits (scores) for each vocab in the tokenizer\n",
    "print(f\"Model predictions (logits) before training:\\n {predictions}\")\n",
    "print()\n",
    "print(f\"Model predictions (probabilities) before training:\\n {torch.softmax(predictions, dim=1)}\")\n",
    "\n",
    "predictions_vocab = list(map(lambda x: id_to_token[x.item()], torch.argmax(torch.softmax(predictions, dim=1), dim=1).cpu()))\n",
    "print(f\"\\nModel predictions (vocab) before training:\\n {predictions_vocab}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model with `Lightning.Trainer.fit()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Create dataloader\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "dataset = TensorDataset(inputs, labels)\n",
    "dataloader = DataLoader(dataset)\n",
    "len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | we        | Embedding        | 10     | train\n",
      "1 | pe        | PositionEncoding | 0      | train\n",
      "2 | attention | Attention        | 12     | train\n",
      "3 | fc        | Linear           | 15     | train\n",
      "4 | loss      | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "37        Trainable params\n",
      "0         Non-trainable params\n",
      "37        Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "551f3e322681454d87c4e42ae660e2cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=30` reached.\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of trainer\n",
    "trainer = L.Trainer(max_epochs=30,\n",
    "                    accelerator=\"auto\")\n",
    "trainer.fit(model,\n",
    "            train_dataloaders=dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model after training with both training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device) # the model will be moved to cpu after calling trainer.fit() by default\n",
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['what', 'is', 'robot', '<EOS>', 'awesome'], ['robot', 'is', 'what', '<EOS>', 'awesome']]\n"
     ]
    }
   ],
   "source": [
    "# Convert the token ids in the training samples to vocabs\n",
    "inputs_vocab = []\n",
    "for input in inputs:\n",
    "    input_vocab = list(map(lambda x: id_to_token[x.item()], input))\n",
    "    inputs_vocab.append(input_vocab)\n",
    "\n",
    "print(inputs_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sample 0:\n",
      "\n",
      "\tInput: what | Pred: is\n",
      "\tInput: is | Pred: robot\n",
      "\tInput: robot | Pred: <EOS>\n",
      "\tInput: <EOS> | Pred: awesome\n",
      "\tInput: awesome | Pred: <EOS>\n",
      "\n",
      "Training sample 1:\n",
      "\n",
      "\tInput: robot | Pred: is\n",
      "\tInput: is | Pred: what\n",
      "\tInput: what | Pred: <EOS>\n",
      "\tInput: <EOS> | Pred: awesome\n",
      "\tInput: awesome | Pred: <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the model after training with both training samples\n",
    "preds_vocab=[]\n",
    "\n",
    "for input in inputs:\n",
    "    # Make a prediction with the model\n",
    "    pred = torch.argmax(model(input.to(device)), dim=1)\n",
    "    # Convert the token ids back to vocabs\n",
    "    preds_vocab.append(list(map(lambda x: id_to_token[x.item()], pred)))\n",
    "\n",
    "# print(preds_vocab)\n",
    "\n",
    "for i, input_pred in enumerate(zip(inputs_vocab, preds_vocab)):\n",
    "    print(f\"Training sample {i}:\\n\")\n",
    "    \n",
    "    input, pred = input_pred\n",
    "    for j in range(len(input)):\n",
    "        print(f\"\\tInput: {input[j]} | Pred: {pred[j]}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
